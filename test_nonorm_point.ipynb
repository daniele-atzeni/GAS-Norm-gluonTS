{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clts2/lib/python3.10/site-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/clts2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import init_folder, load_list_of_elements, get_dataset_and_metadata, get_dataset_from_file\n",
    "\n",
    "import time\n",
    "import optuna\n",
    "import argparse\n",
    "\n",
    "\n",
    "from gluonts.evaluation import make_evaluation_predictions\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.mx.trainer.callback import TrainingHistory\n",
    "from gluonts.mx.distribution import StudentTOutput, MultivariateGaussianOutput\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "\n",
    "from gluonts.mx.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from my_models.gluonts_models.univariate.feedforward_point import SimpleFeedForwardEstimator as FF_gluonts_univariate_point\n",
    "from my_models.gluonts_models.ffn_multivar import SimpleFeedForwardEstimator as FF_gluonts_multivariate\n",
    "from gluonts.mx.model.transformer import TransformerEstimator\n",
    "from gluonts.mx.model.deepar import DeepAREstimator \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "from data_manager import GluonTSDataManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seasonality = {\n",
    "    'nn5_weekly': 52.17857142857143,\n",
    "    'us_births_dataset': 7,\n",
    "    'weather': 7,\n",
    "    'sunspot_without_missing': 7,\n",
    "    'solar_10_minutes': 144,\n",
    "    'hospital': 12,\n",
    "    'rideshare_without_missing': 24,\n",
    "    'fred_md': 12\n",
    "}\n",
    "class Objective:\n",
    "\n",
    "    def __init__( self, MODEL, DATASET_NAME, ctx, DATASET_FILE_FOLDER, multivariate=False):\n",
    "        \n",
    "        data_manager = GluonTSDataManager(DATASET_NAME, multivariate, DATASET_FILE_FOLDER)\n",
    "        self.n_features = data_manager.n_features\n",
    "        self.context_length = data_manager.context_length\n",
    "        self.prediction_length = data_manager.prediction_length\n",
    "        self.train_original = data_manager.train_dataset\n",
    "        self.test = data_manager.test_dataset\n",
    "        self.freq = data_manager.freq\n",
    "        self.seasonality = seasonality[DATASET_NAME]\n",
    "        self.dataset_name = DATASET_NAME\n",
    "\n",
    "        self.model = MODEL\n",
    "        self.multivariate = multivariate\n",
    "        self.ctx = ctx\n",
    "\n",
    "        self.train, test_template = split(self.train_original, offset=-self.prediction_length)\n",
    "        validation = test_template.generate_instances(\n",
    "            prediction_length=self.prediction_length,\n",
    "        )\n",
    "        # Assuming `validation` is a list of (input, output) pairs\n",
    "        validation_data = [\n",
    "            {\n",
    "                \"start\": v[0][\"start\"],  # replace with the actual start time\n",
    "                \"target\": np.concatenate([v[0]['target'], v[1]['target']]),\n",
    "            }\n",
    "            for v in validation\n",
    "        ]\n",
    "\n",
    "        self.validation = ListDataset(validation_data, freq=self.freq)\n",
    "\n",
    "        print(self.model, self.multivariate, self.ctx)\n",
    "\n",
    "\n",
    "    def get_params(self, trial) -> dict:\n",
    "\n",
    "        if self.model == 'feedforward':\n",
    "          return {\n",
    "              \"num_hidden_dimensions\": [trial.suggest_int(\"hidden_dim_{}\".format(i), 10, 100) for i in range(trial.suggest_int(\"num_layers\", 1, 5))],\n",
    "              \"trainer:learning_rate\": trial.suggest_loguniform(\"trainer:learning_rate\", 1e-6, 1e-4),\n",
    "              \"trainer:epochs\": trial.suggest_int(\"trainer:epochs\", 10, 100),\n",
    "          }\n",
    "        elif self.model == 'deepar':\n",
    "           return {\n",
    "              \"num_cells\": trial.suggest_int(\"num_cells\", 10, 100),\n",
    "              \"num_layers\": trial.suggest_int(\"num_layers\", 1, 5),\n",
    "              \"trainer:learning_rate\": trial.suggest_loguniform(\"trainer:learning_rate\", 1e-6, 1e-4),\n",
    "              \"trainer:epochs\": trial.suggest_int(\"trainer:epochs\", 10, 100)\n",
    "           }\n",
    "        elif self.model == 'transformer':\n",
    "          # num_heads must divide model_dim\n",
    "          valid_pairs = [ (i,d) for i in range(10,101) for d in range(1,11) if i%d == 0  ]\n",
    "          model_dim_num_heads_pair = trial.suggest_categorical(\"model_dim_num_heads_pair\", valid_pairs)\n",
    "\n",
    "          return {\n",
    "              \"inner_ff_dim_scale\": trial.suggest_int(\"inner_ff_dim_scale\", 1, 5),\n",
    "              \"model_dim\": model_dim_num_heads_pair[0],\n",
    "              \"embedding_dimension\": trial.suggest_int(\"embedding_dimension\", 1, 10),\n",
    "              \"num_heads\": model_dim_num_heads_pair[1],\n",
    "              \"dropout_rate\": trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5),\n",
    "              \"trainer:learning_rate\": trial.suggest_loguniform(\"trainer:learning_rate\", 1e-6, 1e-4),\n",
    "              \"trainer:epochs\": trial.suggest_int(\"trainer:epochs\", 10, 100),\n",
    "          }\n",
    "\n",
    "\n",
    "    def __call__(self, trial):\n",
    "\n",
    "        params = self.get_params(trial)\n",
    "\n",
    "        return self.train_and_test(params)\n",
    "\n",
    "    def train_and_test(self, params, save=False):\n",
    "\n",
    "      history = TrainingHistory()\n",
    "      if self.model == 'feedforward' and not self.multivariate:\n",
    "        # estimator = SimpleFeedForwardEstimator_nonorm_point(\n",
    "        #     num_hidden_dimensions= params['num_hidden_dimensions'], #num_hidden_dimensions,\n",
    "        #     prediction_length=self.prediction_length,\n",
    "        #     context_length=self.context_length,\n",
    "        #     n_features=self.n_features,\n",
    "        #     trainer=Trainer(ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'], num_batches_per_epoch=100),\n",
    "        # )\n",
    "        estimator = FF_gluonts_univariate_point(\n",
    "            num_hidden_dimensions= params['num_hidden_dimensions'], #num_hidden_dimensions,\n",
    "            prediction_length=self.prediction_length,\n",
    "            context_length=self.context_length,\n",
    "            batch_normalization=True,\n",
    "            mean_scaling=False,\n",
    "            trainer=Trainer(ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                             num_batches_per_epoch=100, callbacks=[history]),\n",
    "        )\n",
    "      elif self.model == 'feedforward' and self.multivariate:\n",
    "        estimator = FF_gluonts_multivariate(\n",
    "            num_hidden_dimensions= params['num_hidden_dimensions'], #num_hidden_dimensions,\n",
    "            prediction_length=self.prediction_length,\n",
    "            context_length=self.context_length,\n",
    "            mean_scaling=False,\n",
    "            # batch_normalization=True,\n",
    "            distr_output=MultivariateGaussianOutput(dim=self.n_features),\n",
    "            trainer=Trainer(ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                             num_batches_per_epoch=100, callbacks=[history]),\n",
    "        )\n",
    "      elif self.model == 'deepar':\n",
    "        estimator = DeepAREstimator(\n",
    "            freq=self.freq,\n",
    "            context_length=self.context_length,\n",
    "            distr_output=StudentTOutput(),\n",
    "            prediction_length=self.prediction_length,\n",
    "            num_cells= params['num_cells'],\n",
    "            num_layers= params['num_layers'],\n",
    "            trainer=Trainer(ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                             num_batches_per_epoch=100, callbacks=[history]),\n",
    "        )\n",
    "      elif self.model == 'transformer':\n",
    "        estimator = TransformerEstimator(\n",
    "            freq=self.freq,\n",
    "            context_length=self.context_length,\n",
    "            prediction_length=self.prediction_length,\n",
    "            inner_ff_dim_scale= params['inner_ff_dim_scale'],\n",
    "            model_dim= params['model_dim'],\n",
    "            embedding_dimension= params['embedding_dimension'],\n",
    "            num_heads= params['num_heads'],\n",
    "            dropout_rate= params['dropout_rate'],\n",
    "            trainer=Trainer(ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                             num_batches_per_epoch=100, callbacks=[history]),\n",
    "        )\n",
    "\n",
    "      ## TRAIN\n",
    "      predictor = estimator.train(self.train, self.validation)\n",
    "      ## EVALUATE\n",
    "      if not save:\n",
    "         test = self.validation\n",
    "      else:\n",
    "         test = self.test\n",
    "      forecast_it, ts_it = make_evaluation_predictions(\n",
    "          dataset=test,  # validation dataset\n",
    "          predictor=predictor,  # predictor\n",
    "          num_samples=100,  # number of sample paths we want for evaluation\n",
    "      )\n",
    "\n",
    "      forecasts = list(forecast_it)\n",
    "\n",
    "      final_forecasts = []\n",
    "      for f in forecasts:\n",
    "          final_forecasts.append(f.median)\n",
    "\n",
    "      mase_metrics = []\n",
    "      for item_id, ts in enumerate(test):\n",
    "        training_data = ts[\"target\"].T[:-self.prediction_length]\n",
    "        ground_truth = ts[\"target\"].T[-self.prediction_length:]\n",
    "\n",
    "        y_pred_naive = np.array(training_data)[:-int(self.seasonality)]\n",
    "        mae_naive = mean_absolute_error(np.array(training_data)[int(self.seasonality):], y_pred_naive, multioutput=\"uniform_average\")\n",
    "\n",
    "        mae_score = mean_absolute_error(\n",
    "            np.array(ground_truth),\n",
    "            final_forecasts[item_id],\n",
    "            sample_weight=None,\n",
    "            multioutput=\"uniform_average\",\n",
    "        )\n",
    "\n",
    "        epsilon = np.finfo(np.float64).eps\n",
    "        if mae_naive == 0:\n",
    "          continue\n",
    "        mase_score = mae_score / np.maximum(mae_naive, epsilon)\n",
    "\n",
    "\n",
    "        mase_metrics.append(mase_score)\n",
    "      if not save:\n",
    "        return np.mean(mase_metrics)\n",
    "      \n",
    "      # make directory called saved_nonorm_{self.model}_{self.dataset_name}\n",
    "      dir_name = f'saved_nonorm_{self.model}_{self.dataset_name}'\n",
    "      os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "      return np.mean(mase_metrics), predictor, dir_name, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-10 14:24:25,913] A new study created in memory with name: no-name-56e9b8f5-24f7-40f5-9552-e8ffd39d0c65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16320/473181749.py:52: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"trainer:learning_rate\": trial.suggest_loguniform(\"trainer:learning_rate\", 1e-6, 1e-4),\n",
      "[14:24:25] /work/mxnet/src/base.cc:79: cuDNN lib mismatch: linked-against version 8101 != compiled-against version 8500.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feedforward False gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 26.15it/s, epoch=1/75, avg_epoch_loss=1.4]\n",
      "4it [00:00, 104.74it/s, epoch=1/75, validation_avg_epoch_loss=1.37]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.05it/s, epoch=2/75, avg_epoch_loss=1.38]\n",
      "4it [00:00, 348.59it/s, epoch=2/75, validation_avg_epoch_loss=1.36]\n",
      "100%|██████████| 100/100 [00:04<00:00, 22.04it/s, epoch=3/75, avg_epoch_loss=1.38]\n",
      "4it [00:00, 49.79it/s, epoch=3/75, validation_avg_epoch_loss=1.38]\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.46it/s, epoch=4/75, avg_epoch_loss=1.37]\n",
      "4it [00:00, 57.57it/s, epoch=4/75, validation_avg_epoch_loss=1.38]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.83it/s, epoch=5/75, avg_epoch_loss=1.37]\n",
      "4it [00:00, 43.33it/s, epoch=5/75, validation_avg_epoch_loss=1.39]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.81it/s, epoch=6/75, avg_epoch_loss=1.36]\n",
      "4it [00:00, 40.67it/s, epoch=6/75, validation_avg_epoch_loss=1.34]\n",
      "100%|██████████| 100/100 [00:04<00:00, 22.18it/s, epoch=7/75, avg_epoch_loss=1.34]\n",
      "4it [00:00, 30.61it/s, epoch=7/75, validation_avg_epoch_loss=1.38]\n",
      "100%|██████████| 100/100 [00:03<00:00, 32.64it/s, epoch=8/75, avg_epoch_loss=1.34]\n",
      "4it [00:00, 374.47it/s, epoch=8/75, validation_avg_epoch_loss=1.34]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.75it/s, epoch=9/75, avg_epoch_loss=1.33]\n",
      "4it [00:00, 155.68it/s, epoch=9/75, validation_avg_epoch_loss=1.39]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.48it/s, epoch=10/75, avg_epoch_loss=1.32]\n",
      "4it [00:00, 27.21it/s, epoch=10/75, validation_avg_epoch_loss=1.32]\n",
      "100%|██████████| 100/100 [00:05<00:00, 17.10it/s, epoch=11/75, avg_epoch_loss=1.31]\n",
      "4it [00:00, 39.07it/s, epoch=11/75, validation_avg_epoch_loss=1.28]\n",
      "100%|██████████| 100/100 [00:03<00:00, 25.41it/s, epoch=12/75, avg_epoch_loss=1.3]\n",
      "4it [00:00, 413.28it/s, epoch=12/75, validation_avg_epoch_loss=1.32]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.09it/s, epoch=13/75, avg_epoch_loss=1.3]\n",
      "4it [00:00, 38.06it/s, epoch=13/75, validation_avg_epoch_loss=1.32]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.59it/s, epoch=14/75, avg_epoch_loss=1.29]\n",
      "4it [00:00, 26.41it/s, epoch=14/75, validation_avg_epoch_loss=1.29]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.97it/s, epoch=15/75, avg_epoch_loss=1.29]\n",
      "4it [00:00, 32.25it/s, epoch=15/75, validation_avg_epoch_loss=1.29]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.92it/s, epoch=16/75, avg_epoch_loss=1.28]\n",
      "4it [00:00, 47.35it/s, epoch=16/75, validation_avg_epoch_loss=1.28]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.35it/s, epoch=17/75, avg_epoch_loss=1.28]\n",
      "4it [00:00, 88.44it/s, epoch=17/75, validation_avg_epoch_loss=1.34]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.88it/s, epoch=18/75, avg_epoch_loss=1.26]\n",
      "4it [00:00, 42.90it/s, epoch=18/75, validation_avg_epoch_loss=1.33]\n",
      "100%|██████████| 100/100 [00:06<00:00, 16.37it/s, epoch=19/75, avg_epoch_loss=1.26]\n",
      "4it [00:00, 33.51it/s, epoch=19/75, validation_avg_epoch_loss=1.26]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.81it/s, epoch=20/75, avg_epoch_loss=1.25]\n",
      "4it [00:00, 50.65it/s, epoch=20/75, validation_avg_epoch_loss=1.24]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.61it/s, epoch=21/75, avg_epoch_loss=1.24]\n",
      "4it [00:00, 34.39it/s, epoch=21/75, validation_avg_epoch_loss=1.29]\n",
      "100%|██████████| 100/100 [00:03<00:00, 32.38it/s, epoch=22/75, avg_epoch_loss=1.24]\n",
      "4it [00:00, 41.61it/s, epoch=22/75, validation_avg_epoch_loss=1.3]\n",
      "100%|██████████| 100/100 [00:03<00:00, 26.47it/s, epoch=23/75, avg_epoch_loss=1.23]\n",
      "4it [00:00, 24.66it/s, epoch=23/75, validation_avg_epoch_loss=1.27]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.95it/s, epoch=24/75, avg_epoch_loss=1.23]\n",
      "4it [00:00, 28.08it/s, epoch=24/75, validation_avg_epoch_loss=1.24]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.75it/s, epoch=25/75, avg_epoch_loss=1.22]\n",
      "4it [00:00, 33.30it/s, epoch=25/75, validation_avg_epoch_loss=1.27]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.20it/s, epoch=26/75, avg_epoch_loss=1.22]\n",
      "4it [00:00, 363.37it/s, epoch=26/75, validation_avg_epoch_loss=1.24]\n",
      "100%|██████████| 100/100 [00:07<00:00, 14.10it/s, epoch=27/75, avg_epoch_loss=1.21]\n",
      "4it [00:00, 97.51it/s, epoch=27/75, validation_avg_epoch_loss=1.31]\n",
      "100%|██████████| 100/100 [00:03<00:00, 29.63it/s, epoch=28/75, avg_epoch_loss=1.21]\n",
      "4it [00:00, 111.22it/s, epoch=28/75, validation_avg_epoch_loss=1.23]\n",
      "100%|██████████| 100/100 [00:04<00:00, 23.58it/s, epoch=29/75, avg_epoch_loss=1.2]\n",
      "4it [00:00, 26.94it/s, epoch=29/75, validation_avg_epoch_loss=1.26]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.52it/s, epoch=30/75, avg_epoch_loss=1.19]\n",
      "4it [00:00, 32.37it/s, epoch=30/75, validation_avg_epoch_loss=1.26]\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.44it/s, epoch=31/75, avg_epoch_loss=1.19]\n",
      "4it [00:00, 32.67it/s, epoch=31/75, validation_avg_epoch_loss=1.26]\n",
      "100%|██████████| 100/100 [00:03<00:00, 33.09it/s, epoch=32/75, avg_epoch_loss=1.18]\n",
      "4it [00:00, 248.33it/s, epoch=32/75, validation_avg_epoch_loss=1.27]\n",
      "100%|██████████| 100/100 [00:02<00:00, 41.35it/s, epoch=33/75, avg_epoch_loss=1.18]\n",
      "4it [00:00, 52.92it/s, epoch=33/75, validation_avg_epoch_loss=1.25]\n",
      "100%|██████████| 100/100 [00:06<00:00, 16.02it/s, epoch=34/75, avg_epoch_loss=1.17]\n",
      "4it [00:00, 54.68it/s, epoch=34/75, validation_avg_epoch_loss=1.24]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.04it/s, epoch=35/75, avg_epoch_loss=1.17]\n",
      "4it [00:00, 49.80it/s, epoch=35/75, validation_avg_epoch_loss=1.23]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.74it/s, epoch=36/75, avg_epoch_loss=1.16]\n",
      "4it [00:00, 28.87it/s, epoch=36/75, validation_avg_epoch_loss=1.22]\n",
      "100%|██████████| 100/100 [00:05<00:00, 19.00it/s, epoch=37/75, avg_epoch_loss=1.16]\n",
      "4it [00:00, 30.69it/s, epoch=37/75, validation_avg_epoch_loss=1.21]\n",
      "100%|██████████| 100/100 [00:05<00:00, 18.09it/s, epoch=38/75, avg_epoch_loss=1.16]\n",
      "4it [00:00, 390.39it/s, epoch=38/75, validation_avg_epoch_loss=1.25]\n",
      "100%|██████████| 100/100 [00:02<00:00, 38.21it/s, epoch=39/75, avg_epoch_loss=1.15]\n",
      "4it [00:00, 53.97it/s, epoch=39/75, validation_avg_epoch_loss=1.2]\n",
      "100%|██████████| 100/100 [00:03<00:00, 32.15it/s, epoch=40/75, avg_epoch_loss=1.15]\n",
      "4it [00:00, 43.93it/s, epoch=40/75, validation_avg_epoch_loss=1.25]\n",
      "100%|██████████| 100/100 [00:07<00:00, 14.21it/s, epoch=41/75, avg_epoch_loss=1.15]\n",
      "4it [00:00, 29.66it/s, epoch=41/75, validation_avg_epoch_loss=1.19]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.83it/s, epoch=42/75, avg_epoch_loss=1.14]\n",
      "4it [00:00, 37.42it/s, epoch=42/75, validation_avg_epoch_loss=1.2]\n",
      "100%|██████████| 100/100 [00:05<00:00, 19.31it/s, epoch=43/75, avg_epoch_loss=1.14]\n",
      "4it [00:00, 40.28it/s, epoch=43/75, validation_avg_epoch_loss=1.17]\n",
      "100%|██████████| 100/100 [00:03<00:00, 25.38it/s, epoch=44/75, avg_epoch_loss=1.13]\n",
      "4it [00:00, 432.61it/s, epoch=44/75, validation_avg_epoch_loss=1.19]\n",
      "100%|██████████| 100/100 [00:05<00:00, 17.60it/s, epoch=45/75, avg_epoch_loss=1.13]\n",
      "4it [00:00, 49.77it/s, epoch=45/75, validation_avg_epoch_loss=1.15]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.58it/s, epoch=46/75, avg_epoch_loss=1.12]\n",
      "4it [00:00, 35.25it/s, epoch=46/75, validation_avg_epoch_loss=1.16]\n",
      "100%|██████████| 100/100 [00:05<00:00, 17.36it/s, epoch=47/75, avg_epoch_loss=1.12]\n",
      "4it [00:00, 70.38it/s, epoch=47/75, validation_avg_epoch_loss=1.21]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.90it/s, epoch=48/75, avg_epoch_loss=1.12]\n",
      "4it [00:00, 28.50it/s, epoch=48/75, validation_avg_epoch_loss=1.21]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.47it/s, epoch=49/75, avg_epoch_loss=1.11]\n",
      "4it [00:00, 130.01it/s, epoch=49/75, validation_avg_epoch_loss=1.17]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.46it/s, epoch=50/75, avg_epoch_loss=1.11]\n",
      "4it [00:00, 48.00it/s, epoch=50/75, validation_avg_epoch_loss=1.14]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.64it/s, epoch=51/75, avg_epoch_loss=1.11]\n",
      "4it [00:00, 27.24it/s, epoch=51/75, validation_avg_epoch_loss=1.13]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.63it/s, epoch=52/75, avg_epoch_loss=1.1]\n",
      "4it [00:00, 32.58it/s, epoch=52/75, validation_avg_epoch_loss=1.23]\n",
      "100%|██████████| 100/100 [00:06<00:00, 16.44it/s, epoch=53/75, avg_epoch_loss=1.1]\n",
      "4it [00:00, 281.39it/s, epoch=53/75, validation_avg_epoch_loss=1.18]\n",
      "100%|██████████| 100/100 [00:05<00:00, 19.22it/s, epoch=54/75, avg_epoch_loss=1.1]\n",
      "4it [00:00, 380.25it/s, epoch=54/75, validation_avg_epoch_loss=1.16]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.58it/s, epoch=55/75, avg_epoch_loss=1.09]\n",
      "4it [00:00, 353.53it/s, epoch=55/75, validation_avg_epoch_loss=1.16]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.62it/s, epoch=56/75, avg_epoch_loss=1.09]\n",
      "4it [00:00, 59.07it/s, epoch=56/75, validation_avg_epoch_loss=1.17]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.36it/s, epoch=57/75, avg_epoch_loss=1.09]\n",
      "4it [00:00, 61.69it/s, epoch=57/75, validation_avg_epoch_loss=1.14]\n",
      "100%|██████████| 100/100 [00:04<00:00, 21.20it/s, epoch=58/75, avg_epoch_loss=1.08]\n",
      "4it [00:00, 55.42it/s, epoch=58/75, validation_avg_epoch_loss=1.13]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.32it/s, epoch=59/75, avg_epoch_loss=1.08]\n",
      "4it [00:00, 74.78it/s, epoch=59/75, validation_avg_epoch_loss=1.18]\n",
      "100%|██████████| 100/100 [00:04<00:00, 21.66it/s, epoch=60/75, avg_epoch_loss=1.08]\n",
      "4it [00:00, 105.14it/s, epoch=60/75, validation_avg_epoch_loss=1.15]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.27it/s, epoch=61/75, avg_epoch_loss=1.08]\n",
      "4it [00:00, 26.85it/s, epoch=61/75, validation_avg_epoch_loss=1.17]\n",
      "100%|██████████| 100/100 [00:09<00:00, 10.19it/s, epoch=62/75, avg_epoch_loss=1.07]\n",
      "4it [00:00, 120.15it/s, epoch=62/75, validation_avg_epoch_loss=1.1]\n",
      "100%|██████████| 100/100 [00:05<00:00, 17.25it/s, epoch=63/75, avg_epoch_loss=1.07]\n",
      "4it [00:00, 234.74it/s, epoch=63/75, validation_avg_epoch_loss=1.17]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.64it/s, epoch=64/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 31.58it/s, epoch=64/75, validation_avg_epoch_loss=1.15]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.12it/s, epoch=65/75, avg_epoch_loss=1.07]\n",
      "4it [00:00, 26.62it/s, epoch=65/75, validation_avg_epoch_loss=1.16]\n",
      "100%|██████████| 100/100 [00:11<00:00,  8.89it/s, epoch=66/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 27.47it/s, epoch=66/75, validation_avg_epoch_loss=1.13]\n",
      "100%|██████████| 100/100 [00:08<00:00, 11.37it/s, epoch=67/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 31.83it/s, epoch=67/75, validation_avg_epoch_loss=1.17]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.16it/s, epoch=68/75, avg_epoch_loss=1.07]\n",
      "4it [00:00, 33.17it/s, epoch=68/75, validation_avg_epoch_loss=1.12]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.35it/s, epoch=69/75, avg_epoch_loss=1.07]\n",
      "4it [00:00, 44.52it/s, epoch=69/75, validation_avg_epoch_loss=1.18]\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.03it/s, epoch=70/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 32.32it/s, epoch=70/75, validation_avg_epoch_loss=1.11]\n",
      "100%|██████████| 100/100 [00:08<00:00, 12.08it/s, epoch=71/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 26.84it/s, epoch=71/75, validation_avg_epoch_loss=1.17]\n",
      "100%|██████████| 100/100 [00:10<00:00,  9.39it/s, epoch=72/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 29.69it/s, epoch=72/75, validation_avg_epoch_loss=1.13]\n",
      "100%|██████████| 100/100 [00:11<00:00,  8.59it/s, epoch=73/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 36.23it/s, epoch=73/75, validation_avg_epoch_loss=1.12]\n",
      "100%|██████████| 100/100 [00:11<00:00,  8.89it/s, epoch=74/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 208.19it/s, epoch=74/75, validation_avg_epoch_loss=1.12]\n",
      "100%|██████████| 100/100 [00:04<00:00, 24.52it/s, epoch=75/75, avg_epoch_loss=1.06]\n",
      "4it [00:00, 222.44it/s, epoch=75/75, validation_avg_epoch_loss=1.16]\n",
      "[I 2024-02-10 14:33:16,492] Trial 0 finished with value: 7.89536941247957 and parameters: {'num_layers': 4, 'hidden_dim_0': 18, 'hidden_dim_1': 65, 'hidden_dim_2': 44, 'hidden_dim_3': 19, 'trainer:learning_rate': 2.490519201895369e-06, 'trainer:epochs': 75}. Best is trial 0 with value: 7.89536941247957.\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME, model_choice, ctx, DATASET_FILE_FOLDER, n_trials = 'nn5_weekly', 'feedforward', 'gpu', None, 1\n",
    "multivariate = False\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "obj = Objective(\n",
    "        model_choice,DATASET_NAME, ctx, DATASET_FILE_FOLDER, multivariate\n",
    "    )\n",
    "study.optimize(\n",
    "    obj,\n",
    "    n_trials=n_trials,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clts2/lib/python3.10/site-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n",
      "[14:23:44] /work/mxnet/src/base.cc:79: cuDNN lib mismatch: linked-against version 8101 != compiled-against version 8500.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.70it/s, epoch=1/1, avg_epoch_loss=3.69e+3]\n",
      "Running evaluation: 414it [00:16, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSE': 101894675.60529852, 'abs_error': 29009996.051239014, 'abs_target_sum': 145558863.59960938, 'abs_target_mean': 7324.822041043146, 'seasonal_error': 336.9046924038305, 'MASE': 12.371873980916131, 'MAPE': 0.8961260776588882, 'sMAPE': 0.40215173498060003, 'MSIS': 494.8749590281817, 'num_masked_target_values': 0.0, 'QuantileLoss[0.1]': 36398130.45469226, 'Coverage[0.1]': 0.5782004830917874, 'QuantileLoss[0.2]': 34551096.897538, 'Coverage[0.2]': 0.5782004830917874, 'QuantileLoss[0.3]': 32704063.340383716, 'Coverage[0.3]': 0.5782004830917874, 'QuantileLoss[0.4]': 30857029.783229448, 'Coverage[0.4]': 0.5782004830917874, 'QuantileLoss[0.5]': 29009996.226075172, 'Coverage[0.5]': 0.5782004830917874, 'QuantileLoss[0.6]': 27162962.668920897, 'Coverage[0.6]': 0.5782004830917874, 'QuantileLoss[0.7]': 25315929.111766625, 'Coverage[0.7]': 0.5782004830917874, 'QuantileLoss[0.8]': 23468895.554612346, 'Coverage[0.8]': 0.5782004830917874, 'QuantileLoss[0.9]': 21621861.997458074, 'Coverage[0.9]': 0.5782004830917874, 'RMSE': 10094.289257065033, 'NRMSE': 1.3780934472542463, 'ND': 0.19930078686954564, 'wQuantileLoss[0.1]': 0.25005780860458665, 'wQuantileLoss[0.2]': 0.23736855347111077, 'wQuantileLoss[0.3]': 0.22467929833763473, 'wQuantileLoss[0.4]': 0.2119900432041588, 'wQuantileLoss[0.5]': 0.1993007880706828, 'wQuantileLoss[0.6]': 0.1866115329372068, 'wQuantileLoss[0.7]': 0.17392227780373082, 'wQuantileLoss[0.8]': 0.1612330226702548, 'wQuantileLoss[0.9]': 0.14854376753677884, 'mean_absolute_QuantileLoss': 29009996.226075172, 'mean_wQuantileLoss': 0.1993007880706828, 'MAE_Coverage': 0.23091116478797635, 'OWA': nan}\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.mx.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from my_models.gluonts_models.univariate.feedforward_point import SimpleFeedForwardEstimator as FF_gluonts_univariate_point\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "\n",
    "# Load the \"m4_hourly\" dataset\n",
    "dataset = get_dataset(\"m4_hourly\")\n",
    "\n",
    "# Define a trainer\n",
    "trainer = Trainer(epochs=1, ctx=\"gpu\")\n",
    "\n",
    "# Create an estimator\n",
    "estimator = FF_gluonts_univariate_point(\n",
    "    num_hidden_dimensions=[10],\n",
    "    prediction_length=dataset.metadata.prediction_length,\n",
    "    context_length=100,\n",
    "    trainer=trainer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "predictor = estimator.train(dataset.train)\n",
    "\n",
    "# Make prediction\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset.test, predictor=predictor, num_samples=100\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = Evaluator()\n",
    "agg_metrics, item_metrics = evaluator(iter(ts_it), iter(forecast_it))\n",
    "\n",
    "print(agg_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clts2/lib/python3.10/site-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n",
      "[14:19:41] /work/mxnet/src/base.cc:79: cuDNN lib mismatch: linked-against version 8101 != compiled-against version 8500.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n",
      "100%|██████████| 50/50 [00:09<00:00,  5.06it/s, epoch=1/1, avg_epoch_loss=5.61]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[712.06976, 3099.666, 1295.6606, 5254.0474, 2758.132, 6739.0635, 41638.887, 32675.268, 38622.543, 460.45834, 33708.035, 9155.648, 617.13306, 182.92618, 15201.203, 1366.4558, 55573.094, 33641.332, 2557.0364, 7539.308, 17865.41, 691.948, 295.54846, 661.96643, 2514.2844, 19049.947, 2125.2705, 4148.7754, 5477.3154, 119475.96, 109065.0, 100502.625, 5663.445, 45603.254, 1368.237, 25294.047, 2050.2466, 2767.7705, 119734.34, 2081.7764, 3539.8167, 3946.435, 3743.1091, 1108.6484, 38104.48, 180.5606, 37146.715, 4879.6606, 16946.076, 38267.387, 427.66965, 2882.23, 2082.9812, 428.73773, 539.51263, 24100.334, 585534.4, 3440.4314, 1225.8464, 980.4658, 3348.35, 449.09, 7932.8125, 1801.3627, 12242.269, 31759.867, 32751.893, 38705.633, 934.28107, 26416.99, 9817.205, 260.66025, 432.19354, 510.5751, 15294.087, 687.97363, 610.6842, 55416.082, 31783.754, 2443.2603, 5563.8267, 16414.518, 995.9561, 525.613, 924.63666, 255.96924, 757.1403, 945.06055, 1527.7343, 16778.69, 1955.0406, 3153.1592, 5775.0386, 105481.625, 96738.55, 4708.3057, 49728.664, 2868.4617, 21916.37, 1051.8599, 2500.7354, 1944.4269, 115786.984, 2140.0884, 2442.4463, 3251.125, 4027.0796, 467.38443, 40553.234, 39306.703, 1355.2277, 5549.3965, 21726.855, 37516.688, 406.3901, 3138.7388, 1607.5298, 311.098, 178.82893, 24466.152, 574099.5, 5681.7764, 1248.3904, 1033.0604, 775.3088, 1074.7091, 1358.4027, 985.2727, 948.3375, 150.89305, 64.08774, 194.58197, 179.39125, 125.49047, 199.42104, 533.19666, 325.1663, 612.3818, 599.06885, 377.85672, 304.675, 459.74707, 354.87924, 626.64246, 1322.0946, 146.79288, 465.43866, 387.5847, 524.3249, 137.31621, 852.0211, 266.03174, 569.8559, 431.21478, 845.70526, 162.62212, 133.06557, 1717.4142, 350.36765, 204.8578, 2678.7468, 191.4649, 341.9127, 2337.1548, 335.54556, 353.77264, 155.15869, 1060.6758, 151.14865, 22.31051, 19.816544, 21.629454, 21.535694, 21.583818, 20.25446, 20.81751, 20.94252, 21.634016, 22.523579, 20.97619, 19.3518, 25.992573, 20.442379, 19.845327, 20.810228, 19.995218, 27.616861, 20.81233, 21.360846, 21.827093, 21.250082, 18.171494, 21.455376, 20.533665, 20.250736, 19.155235, 22.435387, 21.078848, 21.269407, 21.940159, 22.41408, 22.226458, 21.6765, 21.730305, 21.090033, 21.068573, 23.280039, 20.89108, 22.494946, 22.503292, 21.146461, 21.50506, 18.902384, 21.791428, 21.097126, 22.005762, 20.610435, 21.469812, 15.0923605, 22.573673, 20.135397, 25.282167, 15.900731, 24.77138, 21.33461, 20.002714, 21.369368, 21.066813, 20.906462, 21.950417, 22.073027, 21.38546, 16.824522, 17.980768, 18.421602, 18.460554, 18.795393, 22.330307, 20.938093, 28.095984, 22.452087, 14.302143, 21.418531, 23.360716, 21.748564, 21.614286, 20.59757, 19.59381, 21.991663, 20.20797, 21.514242, 21.822058, 23.100746, 30.260977, 21.640076, 15.308011, 20.55846, 21.123728, 21.948908, 22.343699, 22.351116, 14.523649, 23.322756, 21.23559, 21.898932, 22.429237, 18.779049, 23.061869, 23.65946, 21.91201, 24.015837, 23.76125, 21.707626, 20.168821, 18.23194, 16.97945, 18.195078, 19.211935, 18.197527, 17.468431, 20.896887, 15.878444, 21.6531, 20.70719, 22.309652, 19.016476, 16.695274, 22.053345, 27.498158, 22.851223, 22.241552, 20.602962, 19.069208, 21.516039, 21.63717, 22.042444, 22.917835, 22.649939, 20.909239, 20.976498, 21.515766, 19.8648, 20.14678, 21.778645, 22.615406, 22.897377, 22.326403, 21.516596, 22.419325, 22.323488, 20.004265, 23.310482, 24.796, 23.355015, 14.794116, 21.736498, 24.076956, 21.175594, 21.304148, 19.999817, 22.839582, 22.24367, 21.810932, 21.335, 21.235992, 20.064003, 22.102207, 21.767057, 48.900646, 39.80089, 31.263899, 68.528534, 553.1401, 85.78891, 100.870995, 101.73319, 135.04047, 50.540493, 59.09983, 54.84913, 216.49919, 105.85669, 47.298504, 41.633186, 91.500626, 62.71277, 99.62398, 116.857956, 67.89264, 74.79295, 33.06343, 371.18936, 73.82336, 45.19721, 97.2066, 258.1899, 32.298744, 123.20322, 60.73749, 32.049824, 79.09746, 92.13806, 321.24402, 31.857794, 105.41765, 78.33673, 86.26653, 99.65806, 210.39343, 353.58353, 43.471313, 54.001785, 70.52749, 271.469, 47.186913, 161.02286, 98.00022, 44.33209, 41.60577, 93.517395, 144.99701, 61.26991, 32.46206, 55.4595, 55.249817, 69.07635, 43.522186, 101.891396, 53.779156, 29.605349, 83.378296, 56.073868, 33.22473, 140.1516, 97.68942, 34.53972, 358.49167, 109.82932, 84.05565, 67.94658, 44.97614, 53.92803, 47.342525, 30.321016, 189.2124, 80.756065, 527.9764, 229.65817, 81.497116, 112.260025, 107.40669, 79.55723, 39.88521, 87.93556]\n",
      "[712.06976, 3099.666, 1295.6606, 5254.0474, 2758.132, 6739.0635, 41638.887, 32675.268, 38622.543, 460.45834, 33708.035, 9155.648, 617.13306, 182.92618, 15201.203, 1366.4558, 55573.094, 33641.332, 2557.0364, 7539.308, 17865.41, 691.948, 295.54846, 661.96643, 2514.2844, 19049.947, 2125.2705, 4148.7754, 5477.3154, 119475.96, 109065.0, 100502.625, 5663.445, 45603.254, 1368.237, 25294.047, 2050.2466, 2767.7705, 119734.34, 2081.7764, 3539.8167, 3946.435, 3743.1091, 1108.6484, 38104.48, 180.5606, 37146.715, 4879.6606, 16946.076, 38267.387, 427.66965, 2882.23, 2082.9812, 428.73773, 539.51263, 24100.334, 585534.4, 3440.4314, 1225.8464, 980.4658, 3348.35, 449.09, 7932.8125, 1801.3627, 12242.269, 31759.867, 32751.893, 38705.633, 934.28107, 26416.99, 9817.205, 260.66025, 432.19354, 510.5751, 15294.087, 687.97363, 610.6842, 55416.082, 31783.754, 2443.2603, 5563.8267, 16414.518, 995.9561, 525.613, 924.63666, 255.96924, 757.1403, 945.06055, 1527.7343, 16778.69, 1955.0406, 3153.1592, 5775.0386, 105481.625, 96738.55, 4708.3057, 49728.664, 2868.4617, 21916.37, 1051.8599, 2500.7354, 1944.4269, 115786.984, 2140.0884, 2442.4463, 3251.125, 4027.0796, 467.38443, 40553.234, 39306.703, 1355.2277, 5549.3965, 21726.855, 37516.688, 406.3901, 3138.7388, 1607.5298, 311.098, 178.82893, 24466.152, 574099.5, 5681.7764, 1248.3904, 1033.0604, 775.3088, 1074.7091, 1358.4027, 985.2727, 948.3375, 150.89305, 64.08774, 194.58197, 179.39125, 125.49047, 199.42104, 533.19666, 325.1663, 612.3818, 599.06885, 377.85672, 304.675, 459.74707, 354.87924, 626.64246, 1322.0946, 146.79288, 465.43866, 387.5847, 524.3249, 137.31621, 852.0211, 266.03174, 569.8559, 431.21478, 845.70526, 162.62212, 133.06557, 1717.4142, 350.36765, 204.8578, 2678.7468, 191.4649, 341.9127, 2337.1548, 335.54556, 353.77264, 155.15869, 1060.6758, 151.14865, 22.31051, 19.816544, 21.629454, 21.535694, 21.583818, 20.25446, 20.81751, 20.94252, 21.634016, 22.523579, 20.97619, 19.3518, 25.992573, 20.442379, 19.845327, 20.810228, 19.995218, 27.616861, 20.81233, 21.360846, 21.827093, 21.250082, 18.171494, 21.455376, 20.533665, 20.250736, 19.155235, 22.435387, 21.078848, 21.269407, 21.940159, 22.41408, 22.226458, 21.6765, 21.730305, 21.090033, 21.068573, 23.280039, 20.89108, 22.494946, 22.503292, 21.146461, 21.50506, 18.902384, 21.791428, 21.097126, 22.005762, 20.610435, 21.469812, 15.0923605, 22.573673, 20.135397, 25.282167, 15.900731, 24.77138, 21.33461, 20.002714, 21.369368, 21.066813, 20.906462, 21.950417, 22.073027, 21.38546, 16.824522, 17.980768, 18.421602, 18.460554, 18.795393, 22.330307, 20.938093, 28.095984, 22.452087, 14.302143, 21.418531, 23.360716, 21.748564, 21.614286, 20.59757, 19.59381, 21.991663, 20.20797, 21.514242, 21.822058, 23.100746, 30.260977, 21.640076, 15.308011, 20.55846, 21.123728, 21.948908, 22.343699, 22.351116, 14.523649, 23.322756, 21.23559, 21.898932, 22.429237, 18.779049, 23.061869, 23.65946, 21.91201, 24.015837, 23.76125, 21.707626, 20.168821, 18.23194, 16.97945, 18.195078, 19.211935, 18.197527, 17.468431, 20.896887, 15.878444, 21.6531, 20.70719, 22.309652, 19.016476, 16.695274, 22.053345, 27.498158, 22.851223, 22.241552, 20.602962, 19.069208, 21.516039, 21.63717, 22.042444, 22.917835, 22.649939, 20.909239, 20.976498, 21.515766, 19.8648, 20.14678, 21.778645, 22.615406, 22.897377, 22.326403, 21.516596, 22.419325, 22.323488, 20.004265, 23.310482, 24.796, 23.355015, 14.794116, 21.736498, 24.076956, 21.175594, 21.304148, 19.999817, 22.839582, 22.24367, 21.810932, 21.335, 21.235992, 20.064003, 22.102207, 21.767057, 48.900646, 39.80089, 31.263899, 68.528534, 553.1401, 85.78891, 100.870995, 101.73319, 135.04047, 50.540493, 59.09983, 54.84913, 216.49919, 105.85669, 47.298504, 41.633186, 91.500626, 62.71277, 99.62398, 116.857956, 67.89264, 74.79295, 33.06343, 371.18936, 73.82336, 45.19721, 97.2066, 258.1899, 32.298744, 123.20322, 60.73749, 32.049824, 79.09746, 92.13806, 321.24402, 31.857794, 105.41765, 78.33673, 86.26653, 99.65806, 210.39343, 353.58353, 43.471313, 54.001785, 70.52749, 271.469, 47.186913, 161.02286, 98.00022, 44.33209, 41.60577, 93.517395, 144.99701, 61.26991, 32.46206, 55.4595, 55.249817, 69.07635, 43.522186, 101.891396, 53.779156, 29.605349, 83.378296, 56.073868, 33.22473, 140.1516, 97.68942, 34.53972, 358.49167, 109.82932, 84.05565, 67.94658, 44.97614, 53.92803, 47.342525, 30.321016, 189.2124, 80.756065, 527.9764, 229.65817, 81.497116, 112.260025, 107.40669, 79.55723, 39.88521, 87.93556]\n"
     ]
    }
   ],
   "source": [
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "from gluonts.mx.model.deepar import DeepAREstimator\n",
    "from gluonts.mx.model.seq2seq import MQCNNEstimator\n",
    "from gluonts.mx.model.wavenet import WaveNetEstimator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Load the \"m4_hourly\" dataset\n",
    "dataset = get_dataset(\"m4_hourly\")\n",
    "train_entry = next(iter(dataset.train))\n",
    "train_series = to_pandas(train_entry)\n",
    "\n",
    "# Create a DeepAR estimator\n",
    "estimator = DeepAREstimator(\n",
    "    freq=train_series.index.freq.freqstr,\n",
    "    prediction_length=48,\n",
    "    trainer=Trainer(epochs=1, ctx='gpu(0)')\n",
    ")\n",
    "# estimator = MQCNNEstimator(\n",
    "#     freq=train_series.index.freq.freqstr,\n",
    "#     prediction_length=48,\n",
    "#     context_length=\n",
    "#     trainer=Trainer(epochs=1, ctx='cpu', hybridize=False)\n",
    "# )\n",
    "\n",
    "# Train the estimator on the training data\n",
    "predictor = estimator.train(dataset.train)\n",
    "\n",
    "# Use the predictor to make predictions\n",
    "test_entry = next(iter(dataset.test))\n",
    "test_series = to_pandas(test_entry)\n",
    "# forecast = predictor.predict(test_series.index[0:48].values.reshape(1, -1))\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset.test, predictor=predictor, num_samples=100\n",
    ")\n",
    "forecast_list = list(forecast_it)\n",
    "point_forecasts = []\n",
    "for f in forecast_list:\n",
    "    point_forecasts.append(np.mean(f.samples))\n",
    "\n",
    "print(point_forecasts)\n",
    "\n",
    "print(point_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 15.,  13.,  13.,  16.,  26.,  51.,  91., 103., 106.,  89.,  81.,\n",
       "        88.,  80., 112., 110., 102.,  94., 116., 110.,  59.,  57.,  50.,\n",
       "        35.,  24.,  18.,  14.,  19.,  18.,  21.,  62.,  71.,  95., 101.,\n",
       "       110., 103., 104., 112., 138., 119., 121., 157., 145., 122., 100.,\n",
       "        90.,  61.,  56.,  29.,  26.,  20.,  16.,  17.,  24.,  31.,  44.,\n",
       "        62.,  94., 135., 116., 146., 132., 122., 137., 160., 187., 218.,\n",
       "       195., 158., 156.,  97.,  86.,  64.,  48.,  28.,  26.,  18.,  32.,\n",
       "        32.,  54., 100., 150., 200., 330., 404., 470., 496., 480., 536.,\n",
       "       548., 588., 560., 452., 334., 258., 112.,  54.,  18.,  14.,  15.,\n",
       "        26.,  37.,  77., 105., 110., 119.,  90., 111., 118.,  99.,  86.,\n",
       "        99.,  98.,  96.,  90.,  81.,  69.,  49.,  49.,  34.,  27.,  16.,\n",
       "        15.,  11.,  19.,  28.,  60.,  99.,  93., 111., 113., 104.,  73.,\n",
       "        73., 111.,  94.,  92., 103.,  79.,  77.,  64.,  57.,  56.,  36.,\n",
       "        24.,  18.,  11.,  14.,  22.,  32.,  65.,  82.,  94., 120.,  99.,\n",
       "        75.,  89.,  82., 103.,  94.,  90.,  92.,  85.,  81.,  63.,  60.,\n",
       "        51.,  35.,  20.,  16.,  13.,  11.,  19.,  28.,  51., 102., 100.,\n",
       "       109.,  96.,  87.,  71.,  91., 100., 107., 102., 107., 106., 106.,\n",
       "        84.,  58.,  58.,  38.,  24.,  17.,  17.,  14.,  19.,  22.,  53.,\n",
       "        74.,  95., 101.,  95., 104., 111., 106., 129., 119.,  98., 108.,\n",
       "       111., 118., 107.,  78.,  55.,  47.,  34.,  22.,  20.,  18.,  27.,\n",
       "        28.,  30.,  30.,  63.,  74., 101., 119., 122., 122., 148., 132.,\n",
       "       139., 156., 183., 167., 123.,  92.,  93.,  68.,  52.,  28.,  23.,\n",
       "        21.,  17.,  18.,  19.,  29.,  51.,  82., 103., 174., 226., 197.,\n",
       "       190., 262., 231., 259., 266., 273., 186., 176., 128.,  57.,  27.,\n",
       "        15.,  16.,  15.,  20.,  28.,  74., 102., 104., 118., 103., 108.,\n",
       "        94., 100., 103., 100.,  92.,  93., 105.,  97.,  65.,  54.,  59.,\n",
       "        35.,  19.,  14.,  13.,  11.,  15.,  26.,  72.,  81.,  93., 111.,\n",
       "        89.,  91.,  79.,  83.,  94., 105., 106., 120., 109.,  97.,  66.,\n",
       "        54.,  52.,  31.,  32.,  18.,  15.,  14.,  17.,  26.,  70.,  78.,\n",
       "        93., 113.,  93.,  92., 105.,  83.,  93., 102.,  95., 120., 100.,\n",
       "        84.,  82.,  70.,  59.,  48.,  29.,  20.,  14.,  13.,  19.,  32.,\n",
       "        59.,  94.,  91.,  95., 116.,  93.,  86.,  99., 114., 130., 111.,\n",
       "       118., 128., 108.,  89.,  83.,  54.,  39.,  26.,  22.,  14.,  15.,\n",
       "        19.,  30.,  64.,  80.,  94.,  94.,  97.,  98., 112., 110., 164.,\n",
       "       187., 164., 127., 147., 128.,  89.,  98.,  69.,  56.,  33.,  24.,\n",
       "        20.,  14.,  16.,  27.,  29.,  31.,  52.,  77.,  99., 129., 110.,\n",
       "       127., 129., 138., 141., 147., 119., 108.,  92.,  70.,  71.,  56.,\n",
       "        38.,  27.,  24.,  14.,  12.,  19.,  24.,  31.,  44.,  61., 109.,\n",
       "       146., 181., 195., 208., 231., 251., 271., 221., 181., 188., 154.,\n",
       "       119.,  69.,  27.,  19.,  18.,  14.,  22.,  37.,  73.,  91., 106.,\n",
       "       127., 105.,  82.,  92., 105.,  82.,  94., 118.,  99.,  97.,  96.,\n",
       "        60.,  67.,  43.,  29.,  27.,  22.,  12.,  15.,  21.,  27.,  57.,\n",
       "        91., 100., 100.,  87.,  80., 102.,  87., 111., 100., 117., 109.,\n",
       "       103., 100.,  78.,  61.,  51.,  44.,  26.,  20.,  14.,  13.,  18.,\n",
       "        30.,  71.,  89.,  95., 137.,  94., 100., 114., 107., 108., 121.,\n",
       "       117., 105., 117., 101.,  61.,  65.,  66.,  40.,  36.,  22.,  14.,\n",
       "        14.,  16.,  30.,  68.,  90.,  92., 121., 121., 103.,  95., 115.,\n",
       "       134., 140., 142., 150., 115., 101., 100.,  76.,  87.,  63.,  37.,\n",
       "        25.,  15.,  15.,  22.,  32.,  63.,  91.,  87., 101.,  98., 104.,\n",
       "       122., 138., 181., 205., 191., 178., 163., 169., 146., 141., 117.,\n",
       "       111.,  72.,  47.,  27.,  22.,  16.,  26.,  34.,  51.,  79.,  87.,\n",
       "       141., 163., 176., 194., 247., 254., 285., 288., 301., 285., 243.,\n",
       "       168., 126.,  88.,  47.,  26.,  20.,  15.,  19.,  21.,  23.,  46.,\n",
       "        53.,  80., 129., 175., 199., 232., 322., 345., 301., 303., 281.,\n",
       "       260., 247., 221., 166., 100.,  35.,  31.,  24.,  14.,  18.,  34.,\n",
       "        72.,  94., 103., 116., 118., 118., 107., 114., 136., 140., 123.,\n",
       "       111., 110.,  82.,  74.,  59.,  44.,  34.,  21.,  14.,  12.,  19.,\n",
       "        17.,  28.,  61.,  85., 102., 110., 102.,  85., 108.,  74., 108.,\n",
       "       123., 104.,  94.,  80., 102.,  71.,  50.,  59.,  43.,  24.,  17.,\n",
       "        20.,  17.,  24.,  34.,  55.,  78.,  85., 132., 108.,  97.,  93.,\n",
       "       107., 104., 112., 113., 110.,  87.,  88.,  88.,  57.,  56.,  43.,\n",
       "        22.,  22.,  15.,  13.,  16.,  27.,  57.,  78., 107., 121.,  90.,\n",
       "       100.,  88., 109.,  99., 102., 106., 105.,  81.,  92.,  67.,  58.,\n",
       "        55.,  34.,  17.,  17.,  13.,  17.,  21.,  28.,  45.,  76.,  70.,\n",
       "       106., 101.,  90., 101.,  95., 116., 120., 109.,  93.,  98.,  86.,\n",
       "        78.,  57.,  51.,  42.,  33.,  19.,  16.,  14.,  13.,  22.,  26.,\n",
       "        40.,  62.,  78., 109., 119., 140., 121., 126., 150., 166., 199.,\n",
       "       197., 224., 193., 149., 121.,  70.,  36.,  22.,  14.,  14.,  15.,\n",
       "        20.,  26.,  33.,  42.,  68.,  81., 105., 129., 154., 184., 181.,\n",
       "       238., 274., 276., 259., 273., 222., 183., 140.,  57.,  29.,  21.,\n",
       "        13.,  20.,  19.,  21.,  33.,  46.,  77., 128., 192., 231., 260.,\n",
       "       269., 285., 267., 200., 162., 289., 205., 147., 123.,  65.,  26.,\n",
       "        17.,  17.,  12.,  23.,  37.,  81.,  94., 115., 134., 121., 137.,\n",
       "       124., 118., 111., 145., 115., 114.,  87.,  94.,  59.,  49.,  38.,\n",
       "        34.,  21.,  12.,  22.,  15.,  21.,  21.,  70.,  77.,  98., 102.,\n",
       "        95., 113.,  81.,  84.,  69.,  90., 110.,  97.,  76.,  83.,  67.,\n",
       "        63.,  53.,  38.,  20.,  12.,  13.,  14.,  20.,  34.,  62.,  78.,\n",
       "        94., 100.,  84.,  90.,  90.,  70.,  84., 106., 104., 132., 110.,\n",
       "       116.,  82.,  62.,  63.,  41.,  51.,  26.,  18.,  13.,  23.,  28.,\n",
       "        54.,  85.,  96.,  89., 105.,  85.,  90., 108., 114., 143., 120.,\n",
       "       126., 126., 128.,  77.,  63.,  54.,  42.,  33.,  24.,  20.,  14.,\n",
       "        15.,  24.,  32.,  32.,  62.,  82.,  81., 119., 124., 147., 121.,\n",
       "       177., 139., 188., 216., 224., 181., 188., 145.,  93.,  68.,  31.,\n",
       "        20.,  20.,  16.,  16.,  20.,  29.,  50.,  89.,  96., 156., 233.,\n",
       "       192., 204., 265., 226., 193., 184., 160., 147., 115.,  82.,  56.,\n",
       "        29.,  15.,  16.,  17.,  19.,  38.,  78., 114., 111., 102., 103.,\n",
       "       116.,  96.,  85.,  88.,  89., 117., 128., 105.,  65.,  48.,  41.,\n",
       "        35.,  26.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = data['target']\n",
    "target[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.common import ListDataset\n",
    "def difference(dataset):\n",
    "    differenced_dataset = []\n",
    "    for data in dataset:\n",
    "        target = data['target']\n",
    "        differenced_target = target[1:] - target[:-1]\n",
    "        differenced_data = {**data, 'target': differenced_target}\n",
    "        differenced_dataset.append(differenced_data)\n",
    "    return differenced_dataset\n",
    "\n",
    "train_dataset = difference(dataset.train)\n",
    "test_dataset = difference(dataset.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': array([ -19.,    0.,  -27.,  -48.,  -68.,  -21.,  -27.,  -13.,  -12.,\n",
       "          13.,   14.,   23.,   35.,   38.,   61.,   56.,   56.,   49.,\n",
       "          40.,   23.,   16.,   12.,    2.,  -32.,  -53.,  -14.,  -49.,\n",
       "         -75.,  -58.,  -65.,  -25.,  -24.,   -6.,   -3.,   16.,   21.,\n",
       "          30.,   55.,   45.,   41.,   23.,  -22.,   -5.,    6.,   23.,\n",
       "          32.,   14.,    4.,  -27.,    1.,  -29.,  -59.,  -52.,  -40.,\n",
       "         -29.,   -5.,  -22.,    2.,   14.,   -3.,   34.,   56.,   68.,\n",
       "          65.,   47.,   46.,   27.,   27.,   11.,    1.,  -19.,  -31.,\n",
       "         -53.,  -20.,  -42.,  -48.,  -55.,  -40.,  -37.,  -23.,  -16.,\n",
       "          -7.,    4.,   -3.,   42.,   64.,   75.,   51.,   43.,    2.,\n",
       "          11.,   15.,    8.,   -4.,  -27.,  -63.,  -54.,  -29.,  -36.,\n",
       "         -21.,  -41.,  -38.,  -32.,  -14.,  -11.,    3.,   -2.,    1.,\n",
       "          33.,   45.,   29.,    7.,    0.,    7.,   11.,   14.,   16.,\n",
       "          25.,   10.,  -19.,  -10.,   11.,  -31.,  -44.,  -51.,  -31.,\n",
       "         -25.,  -13.,   -8.,   13.,   12.,   28.,   25.,   29.,   44.,\n",
       "          58.,   50.,   53.,   43.,   32.,   27.,   19.,    0.,   -7.,\n",
       "         -45.,  -14.,  -59.,  -67.,  -72.,  -49.,  -31.,  -20.,  -12.,\n",
       "          -1.,   13.,   22.,   27.,   59.,   62.,   72.,   53.,   54.,\n",
       "          33.,   28.,    9.,   14.,  -15.,  -23.,  -37.,  -23.,  -58.,\n",
       "         -75.,  -78.,  -48.,  -35.,  -20.,  -22.,    6.,    6.,   21.,\n",
       "          32.,   57.,   74.,   64.,   59.,   41.,   43.,    8.,   10.,\n",
       "         -14.,  -19.,  -31.,  -38.,  -18.,  -52.,  -65.,  -69.,  -47.,\n",
       "         -36.,  -22.,  -12.,    3.,   20.,    7.,   42.,   48.,   63.,\n",
       "          71.,   61.,   46.,   25.,   23.,    2.,   -7.,  -17.,  -31.,\n",
       "         -31.,  -18.,  -46.,  -73.,  -57.,  -58.,  -30.,  -20.,  -17.,\n",
       "          -2.,    7.,   24.,   37.,   63.,   62.,   71.,   56.,   46.,\n",
       "          33.,   19.,   10.,    1.,  -14.,  -24.,  -46.,  -18.,  -48.,\n",
       "         -64.,  -50.,  -65.,  -29.,  -27.,  -17.,   -7.,   -3.,    6.,\n",
       "          41.,   83.,   77.,   80.,   50.,   49.,   27.,   16.,   16.,\n",
       "           1.,  -10.,  -32.,  -43.,  -24.,  -49.,  -63.,  -68.,  -46.,\n",
       "         -35.,  -28.,  -21.,  -11.,  -10.,    5.,   59.,   92.,   94.,\n",
       "          64.,   48.,   32.,   30.,   20.,    4.,  -12.,  -15.,  -33.,\n",
       "         -60.,  -14.,  -52.,  -75.,  -63.,  -48.,  -34.,  -23.,  -11.,\n",
       "          -4.,   21.,   18.,   40.,   60.,   85.,   77.,   58.,   34.,\n",
       "          18.,  -13.,  -10.,   -6.,    6.,  -19.,  -34.,  -19.,  -52.,\n",
       "         -69.,  -70.,  -44.,  -29.,  -23.,   -9.,    6.,    6.,   28.,\n",
       "          36.,   52.,   67.,   67.,   58.,   42.,   18.,    8.,   -2.,\n",
       "          10.,   -3.,   -6.,  -38.,  -13.,  -53.,  -70.,  -75.,  -35.,\n",
       "         -40.,  -16.,  -13.,   -5.,   16.,   14.,   32.,   48.,   71.,\n",
       "          61.,   54.,   46.,   19.,   30.,   15.,   15.,  -11.,  -34.,\n",
       "         -48.,  -25.,  -60.,  -72.,  -73.,  -40.,  -38.,  -21.,   -7.,\n",
       "          -5.,   16.,   17.,   25.,   44.,   49.,   72.,   49.,   47.,\n",
       "          32.,   30.,    9.,   -5.,  -22.,  -26.,  -32.,  -25.,  -45.,\n",
       "         -66.,  -76.,  -38.,  -37.,  -30.,  -13.,    8.,    2.,   21.,\n",
       "          37.,   56.,   73.,   78.,   57.,   37.,   26.,   18.,  -11.,\n",
       "          -6.,  -33.,  -31.,  -48.,  -10.,  -48.,  -63.,  -54.,  -52.,\n",
       "         -36.,  -16.,  -27.,   -3.,    2.,    7.,   42.,   80.,   81.,\n",
       "          85.,   69.,   49.,   27.,   16.,   18.,   -6.,   -7.,  -35.,\n",
       "         -45.,  -23.,  -60.,  -60.,  -67.,  -51.,  -37.,  -25.,  -19.,\n",
       "         -21.,    6.,   -6.,   54.,  101.,   84.,   70.,   59.,   40.,\n",
       "           7.,  -15.,  -32.,  -52.,  -39.,  -40.,  -28.,   -9.,  -40.,\n",
       "         -53.,  -56.,  -35.,  -27.,  -18.,  -11.,   12.,   17.,   19.,\n",
       "          34.,   54.,   61.,   85.,   64.,   59.,   46.,   18.,   19.,\n",
       "           9.,   -9.,  -25.,  -45.,  -18.,  -54.,  -70.,  -81.,  -37.,\n",
       "         -34.,  -32.,  -17.,    8.,    8.,   18.,   33.,   58.,   68.,\n",
       "          73.,   64.,   46.,   26.,   -6.,  -16.,  -27.,  -30.,  -31.,\n",
       "         -35.,  -22.,  -54.,  -55.,  -63.,  -41.,  -28.,  -21.,   -2.,\n",
       "          -2.,   18.,   14.,   39.,   56.,   69.,   71.,   73.,   50.,\n",
       "          31.,    4.,    3.,  -27.,  -22.,  -26.,  -43.,  -11.,  -52.,\n",
       "         -68.,  -58.,  -44.,  -29.,  -19.,  -12.,    1.,   20.,   13.,\n",
       "          38.,   54.,   73.,   68.,   50.,   33.,   -1.,   -5.,  -16.,\n",
       "          -9.,  -17.,  -40.,  -12.,  -24.,  -41.,  -60.,  -51.,  -44.,\n",
       "         -29.,  -15.,   -9.,    0.,   19.,   19.,   32.,   46.,   74.,\n",
       "          65.,   57.,   46.,   30.,  -18.,  -36.,  -31.,  -14.,  -19.,\n",
       "         -30.,  -12.,  -44.,  -47.,  -50.,  -45.,  -28.,  -21.,  -16.,\n",
       "          -3.,    6.,   -6.,   49.,   64.,   79.,   68.,   77.,   32.,\n",
       "          33.,    7.,    2.,    3.,  -15.,  -28.,  -45.,  -11.,  -60.,\n",
       "         -57.,  -64.,  -45.,  -37.,  -27.,  -12.,  -15.,    1.,   -8.,\n",
       "          60.,   78.,   71.,   56.,   40.,   49.,   36.,   23.,    8.,\n",
       "          13.,   -5.,  -36.,  -51.,  -27.,  -60.,  -76.,  -66.,  -54.,\n",
       "         -27.,  -24.,  -11.,    1.,   18.,   18.,   40.,   55.,   66.,\n",
       "          86.,   69.,   48.,   38.,   21.,   11.,   16.,   -2.,  -26.,\n",
       "         -40.,  -18.,  -64.,  -85., -114.,   -6.,  -39.,  -22.,  -14.,\n",
       "          -3.,   21.,    5.,   39.,   52.,   76.,   84.,   68.,   52.,\n",
       "          35.,   16.,    6.,    6.,   -7.,  -32.,  -25.,  -33.,  -60.,\n",
       "         -78.,  -73.,  -55.,  -34.,  -25.,  -15.,   -2.,   21.,    5.,\n",
       "          42.,   51.,   70.,   85.,   76.,   41.,   12.,  -11.,  -32.,\n",
       "         -27.,  -30.,   -6.,  -32.,  -13.,  -55.], dtype=float32),\n",
       " 'start': Period('1750-01-01 00:00', 'H'),\n",
       " 'feat_static_cat': array([0], dtype=int32),\n",
       " 'item_id': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "for i in range(mx.context.num_gpus()):\n",
    "    free_mem, total_mem = mx.context.gpu_memory_info(i)\n",
    "    print(f\"GPU {i} memory: free={free_mem / (1024 * 1024)}MB, total={total_mem / (1024 * 1024)}MB\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clts2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
