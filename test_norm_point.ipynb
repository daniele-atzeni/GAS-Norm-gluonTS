{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_dataset_from_file\n",
    "from run_experiment.gas_experiment import run_gas_experiment\n",
    "from default_parameters import *\n",
    "\n",
    "from gluonts.mx.distribution import StudentTOutput, MultivariateGaussianOutput\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator, MultivariateEvaluator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.mx.trainer.callback import TrainingHistory\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "import mxnet as mx\n",
    "from my_models.gluonts_models.univariate.feedforward_linear_means._estimator import (\n",
    "    SimpleFeedForwardEstimator as FF_gluonts_univariate_linear,\n",
    ")\n",
    "from my_models.gluonts_models.univariate.feedforward_gas_means._estimator import (\n",
    "    SimpleFeedForwardEstimator as FF_gluonts_univariate_gas,\n",
    ")\n",
    "from my_models.gluonts_models.univariate.feedforward_gas_means_point._estimator import (\n",
    "    SimpleFeedForwardEstimator as FF_gluonts_univariate_gas_point,\n",
    ")\n",
    "from my_models.gluonts_models.feedforward_multivariate_linear_means._estimator import (\n",
    "    SimpleFeedForwardEstimator as FF_gluonts_multivariate_linear,\n",
    ")\n",
    "# from my_models.gluonts_models.multivariate_feedforward_gas_means._estimator import (\n",
    "#     SimpleFeedForwardEstimator as FF_gluonts_multivariate_gas,\n",
    "# )\n",
    "\n",
    "from my_models.gluonts_models.univariate.deepar_gas_means._estimator import (\n",
    "    DeepAREstimator as DeepAR_gluonts_univariate_gas,\n",
    ")\n",
    "\n",
    "from my_models.gluonts_models.univariate.transformer_gas_means._estimator import (\n",
    "    TransformerEstimator as Transformer_gluonts_gas_means,\n",
    ")\n",
    "\n",
    "from run_experiment.dl_model_experiment import GasHybridBlock\n",
    "from normalizer import GASNormalizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import time\n",
    "import optuna\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seasonality = {\n",
    "    'nn5_weekly': 52.17857142857143,\n",
    "    'us_births_dataset': 7,\n",
    "    'weather': 7,\n",
    "    'sunspot_without_missing': 7,\n",
    "    'solar_10_minutes': 144,\n",
    "    'hospital': 12,\n",
    "    'rideshare_without_missing': 24,\n",
    "    'fred_md': 12\n",
    "}\n",
    "class Objective:\n",
    "\n",
    "    def __init__( self, MODEL, DATASET_NAME, ctx, DATASET_FILE_FOLDER, multivariate=False, mean_str=0.1, var_str=0.1):\n",
    "        # self.train, self.test, self.freq, self.seasonality = get_dataset_from_file(f'{ROOT_FOLDER}/tsf_data/{DATASET_NAME}',prediction_length,context_length)\n",
    "        self.model = MODEL\n",
    "        self.multivariate = multivariate\n",
    "        self.ctx = ctx\n",
    "        self.dataset_name = DATASET_NAME\n",
    "\n",
    "        self.seasonality = seasonality[DATASET_NAME]\n",
    "\n",
    "        DATASET_TYPE = \"gluonts\"  # \"synthetic\"\n",
    "        DATASET_PARAMS = real_world_data_params  # synthetic_generation_params\n",
    "        DATASET_PARAMS[\"multivariate\"] = self.multivariate\n",
    "        # DATASET_FILE_FOLDER = None #'tsf_data'\n",
    "\n",
    "        NORMALIZER_NAME = \"gas_t_student\"  # \"gas_simple_gaussian\", \"gas_complex_gaussian\"\n",
    "        NORMALIZER_INITIAL_GUESSES = gas_t_stud_initial_guesses  # gas_{name}_*\n",
    "        NORMALIZER_BOUNDS = gas_t_stud_bounds\n",
    "        NORMALIZER_PARAMS = gas_t_stud_params\n",
    "        NORMALIZER_PARAMS['mean_strength'] = mean_str\n",
    "        NORMALIZER_PARAMS['var_strength'] = var_str\n",
    "\n",
    "        self.mean_str = mean_str\n",
    "        self.var_str = var_str\n",
    "\n",
    "\n",
    "        MEAN_LAYER_NAME = \"gas\"  # TODO: gas\n",
    "        MEAN_LAYER_PARAMS = gas_mean_layer_params\n",
    "\n",
    "        DL_MODEL_LIBRARY = \"gluonts\"  # \"torch\"\n",
    "        DL_MODEL_NAME = MODEL  # TODO: \"transformer\"\n",
    "        if self.model == 'feedforward':\n",
    "            DL_MODEL_PARAMS = gluonts_feedforward_params\n",
    "        elif self.model == 'transformer':\n",
    "            DL_MODEL_PARAMS = gluonts_transformer_params\n",
    "        elif self.model == 'deepar':\n",
    "            DL_MODEL_PARAMS = gluonts_deepar_params\n",
    "\n",
    "        N_TRAINING_SAMPLES = 5000\n",
    "        N_TEST_SAMPLES = 1000\n",
    "\n",
    "        ROOT_FOLDER = (\n",
    "            f\"RESULTS_{DATASET_NAME}_{self.model}_{NORMALIZER_NAME}_{MEAN_LAYER_NAME}_{DL_MODEL_LIBRARY}_{mean_str}_{var_str}_POINTTEST\"\n",
    "        )\n",
    "        if DATASET_PARAMS[\"multivariate\"]:\n",
    "            ROOT_FOLDER += \"_multivariate\"\n",
    "\n",
    "        # run for the first time to get all the means and stuff\n",
    "        training_params = run_gas_experiment(\n",
    "          DATASET_NAME,\n",
    "          DATASET_TYPE,\n",
    "          DATASET_PARAMS,\n",
    "          ROOT_FOLDER,\n",
    "          NORMALIZER_NAME,\n",
    "          NORMALIZER_INITIAL_GUESSES,\n",
    "          NORMALIZER_BOUNDS,\n",
    "          MEAN_LAYER_NAME,\n",
    "          DL_MODEL_LIBRARY,\n",
    "          DL_MODEL_NAME,\n",
    "          DATASET_FILE_FOLDER,\n",
    "          NORMALIZER_PARAMS,\n",
    "          MEAN_LAYER_PARAMS,\n",
    "          DL_MODEL_PARAMS,\n",
    "          N_TRAINING_SAMPLES,\n",
    "          N_TEST_SAMPLES,\n",
    "        )\n",
    "\n",
    "        # get the parameters needed to run the model part\n",
    "\n",
    "        self.n_features, self.context_length, self.prediction_length, self.freq, self.dataset, self.mean_layer, self.dl_model_name, self.dl_model_params, self.folders = training_params\n",
    "\n",
    "        self.train_original, self.test = self.dataset\n",
    "\n",
    "\n",
    "        self.train, test_template = split(self.train_original, offset=-self.prediction_length)\n",
    "        validation = test_template.generate_instances(\n",
    "            prediction_length=self.prediction_length,\n",
    "        )\n",
    "        # Assuming `validation` is a list of (input, output) pairs\n",
    "        validation_data = [\n",
    "            {\n",
    "                \"start\": v[0][\"start\"],  # replace with the actual start time\n",
    "                \"target\": np.concatenate([v[0]['target'], v[1]['target']]),\n",
    "                \"feat_dynamic_real\": v[0][\"feat_dynamic_real\"],\n",
    "                \"feat_static_real\": v[0][\"feat_static_real\"],\n",
    "            }\n",
    "            for v in validation\n",
    "        ]\n",
    "\n",
    "        self.validation = ListDataset(validation_data, freq=self.freq)\n",
    "\n",
    "    def get_params(self, trial) -> dict:\n",
    "\n",
    "        if self.model == 'feedforward':\n",
    "          return {\n",
    "              \"num_hidden_dimensions\": [trial.suggest_int(\"hidden_dim_{}\".format(i), 10, 100) for i in range(trial.suggest_int(\"num_layers\", 1, 5))],\n",
    "              \"trainer:learning_rate\": trial.suggest_loguniform(\"trainer:learning_rate\", 1e-5, 1e-1),\n",
    "              \"trainer:epochs\": trial.suggest_int(\"trainer:epochs\", 10, 100),\n",
    "          }\n",
    "        elif self.model == 'deepar':\n",
    "           return {\n",
    "              \"num_cells\": trial.suggest_int(\"num_cells\", 10, 100),\n",
    "              \"num_layers\": trial.suggest_int(\"num_layers\", 1, 10), # was 1, 5\n",
    "              \"trainer:learning_rate\": trial.suggest_loguniform(\"trainer:learning_rate\", 1e-6, 1e-3), # was 1e-6, 1e-4\n",
    "              \"trainer:epochs\": trial.suggest_int(\"trainer:epochs\", 10, 100)\n",
    "           }\n",
    "        elif self.model == 'transformer':\n",
    "          # num_heads must divide model_dim\n",
    "          valid_pairs = [ (i,d) for i in range(10,101) for d in range(1,11) if i%d == 0  ]\n",
    "          model_dim_num_heads_pair = trial.suggest_categorical(\"model_dim_num_heads_pair\", valid_pairs)\n",
    "\n",
    "          return {\n",
    "              \"inner_ff_dim_scale\": trial.suggest_int(\"inner_ff_dim_scale\", 1, 5),\n",
    "              \"model_dim\": model_dim_num_heads_pair[0],\n",
    "              \"embedding_dimension\": trial.suggest_int(\"embedding_dimension\", 1, 10),\n",
    "              \"num_heads\": model_dim_num_heads_pair[1],\n",
    "              \"dropout_rate\": trial.suggest_uniform(\"dropout_rate\", 0.0, 0.5),\n",
    "              \"trainer:learning_rate\": trial.suggest_loguniform(\"trainer:learning_rate\", 1e-5, 1e-1),\n",
    "              \"trainer:epochs\": trial.suggest_int(\"trainer:epochs\", 10, 100),\n",
    "          }\n",
    "\n",
    "    def __call__(self, trial):\n",
    "\n",
    "        params = self.get_params(trial)\n",
    "\n",
    "        return self.train_and_test(params)\n",
    "\n",
    "    def train_and_test(self, params, save=False):\n",
    "\n",
    "        history = TrainingHistory()\n",
    "        trained_mean_layer = self.mean_layer\n",
    "        if isinstance(trained_mean_layer, LinearRegression):\n",
    "            mean_layer = mx.gluon.nn.HybridSequential()\n",
    "            mean_layer.add(\n",
    "                mx.gluon.nn.Dense(\n",
    "                    units=self.prediction_length * self.n_features,\n",
    "                    weight_initializer=mx.init.Constant(trained_mean_layer.coef_),\n",
    "                    bias_initializer=mx.init.Constant(trained_mean_layer.intercept_),  # type: ignore # bias is a numpy array, don't know the reasons for this typing error\n",
    "                )\n",
    "            )\n",
    "            mean_layer.add(\n",
    "                mx.gluon.nn.HybridLambda(\n",
    "                    lambda F, o: F.reshape(\n",
    "                        o, (-1, self.prediction_length * self.n_features)\n",
    "                    )  # no need for that but just to be sure\n",
    "                )\n",
    "            )\n",
    "        elif isinstance(trained_mean_layer, GASNormalizer):\n",
    "            mean_layer = GasHybridBlock(trained_mean_layer, self.n_features, self.prediction_length)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown mean layer type: {type(trained_mean_layer)} {trained_mean_layer}\"\n",
    "            )\n",
    "\n",
    "        # freeze the parameters\n",
    "        for param in mean_layer.collect_params().values():\n",
    "            param.grad_req = \"null\"\n",
    "\n",
    "        if self.model == 'feedforward' and self.multivariate:\n",
    "            if isinstance(trained_mean_layer, GASNormalizer):\n",
    "                estimator = FF_gluonts_multivariate_gas(\n",
    "                    mean_layer,\n",
    "                    self.n_features,\n",
    "                    MultivariateGaussianOutput(dim=self.n_features),\n",
    "                    prediction_length=self.prediction_length,\n",
    "                    context_length=self.context_length,\n",
    "                    num_hidden_dimensions= params['num_hidden_dimensions'], #num_hidden_dimensions,\n",
    "                    trainer=Trainer(hybridize=False,ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                                     num_batches_per_epoch=100, callbacks=[history]),\n",
    "                    \n",
    "                )\n",
    "            else:\n",
    "                estimator = FF_gluonts_multivariate_linear(\n",
    "                    mean_layer,\n",
    "                    self.n_features,\n",
    "                    MultivariateGaussianOutput(dim=self.n_features),\n",
    "                    prediction_length=self.prediction_length,\n",
    "                    context_length=self.context_length,\n",
    "                    num_hidden_dimensions= params['num_hidden_dimensions'], #num_hidden_dimensions,\n",
    "                    trainer=Trainer(hybridize=False,ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                                     num_batches_per_epoch=100, callbacks=[history]),\n",
    "                )\n",
    "        elif self.model == 'feedforward' and not self.multivariate:\n",
    "            if isinstance(trained_mean_layer, GASNormalizer):\n",
    "                # estimator = FF_gluonts_univariate_gas(\n",
    "                #     mean_layer,\n",
    "                #     distr_output=StudentTOutput(),\n",
    "                #     prediction_length=self.prediction_length,\n",
    "                #     context_length=self.context_length,\n",
    "                #     num_hidden_dimensions= params['num_hidden_dimensions'], #num_hidden_dimensions,\n",
    "                #     trainer=Trainer(hybridize=False,ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                #                      num_batches_per_epoch=100, callbacks=[history]),\n",
    "                # )\n",
    "                estimator = FF_gluonts_univariate_gas_point(\n",
    "                    mean_layer,\n",
    "                    distr_output=StudentTOutput(),\n",
    "                    prediction_length=self.prediction_length,\n",
    "                    context_length=self.context_length,\n",
    "                    num_hidden_dimensions= params['num_hidden_dimensions'], #num_hidden_dimensions,\n",
    "                    trainer=Trainer(hybridize=False,ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                                     num_batches_per_epoch=100, callbacks=[history]),\n",
    "                )\n",
    "            else:\n",
    "                estimator = FF_gluonts_univariate_linear(\n",
    "                    mean_layer,\n",
    "                    distr_output=StudentTOutput(),\n",
    "                    prediction_length=self.prediction_length,\n",
    "                    context_length=self.context_length,\n",
    "                    trainer=Trainer(hybridize=False,ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                                     num_batches_per_epoch=100, callbacks=[history]),\n",
    "                )\n",
    "        elif self.model == 'transformer':\n",
    "          estimator = Transformer_gluonts_gas_means(\n",
    "              mean_layer,\n",
    "              freq=self.freq,\n",
    "              context_length=self.context_length,\n",
    "              prediction_length=self.prediction_length,\n",
    "              inner_ff_dim_scale= params['inner_ff_dim_scale'],\n",
    "              model_dim= params['model_dim'],\n",
    "              embedding_dimension= params['embedding_dimension'],\n",
    "              num_heads= params['num_heads'],\n",
    "              dropout_rate= params['dropout_rate'],\n",
    "              trainer=Trainer(hybridize=False,ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                                     num_batches_per_epoch=100, callbacks=[history]),\n",
    "          )\n",
    "        elif self.model == 'deepar' and not self.multivariate and isinstance(trained_mean_layer, GASNormalizer):\n",
    "          estimator = DeepAR_gluonts_univariate_gas(\n",
    "              mean_layer,\n",
    "              freq=self.freq,\n",
    "              distr_output=StudentTOutput(),\n",
    "              context_length=self.context_length,\n",
    "              prediction_length=self.prediction_length,\n",
    "            #   num_cells= params['num_cells'],\n",
    "            #   num_layers= params['num_layers'],\n",
    "              trainer=Trainer(hybridize=False,ctx=self.ctx,epochs=params['trainer:epochs'], learning_rate=params['trainer:learning_rate'],\n",
    "                                     num_batches_per_epoch=100, callbacks=[history]),\n",
    "            #   trainer=Trainer(ctx=self.ctx,epochs=50, learning_rate=1e-4, num_batches_per_epoch=100),\n",
    "          )\n",
    "\n",
    "        ## TRAIN\n",
    "        predictor = estimator.train(self.train, self.validation)\n",
    "        ## EVALUATE\n",
    "        if not save:\n",
    "            test = self.validation\n",
    "        else:\n",
    "            test = self.test\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset=test,  # test dataset\n",
    "            predictor=predictor,  # predictor\n",
    "            num_samples=100,  # number of sample paths we want for evaluation\n",
    "        )\n",
    "\n",
    "        forecasts = list(forecast_it)\n",
    "\n",
    "        final_forecasts = []\n",
    "        for f in forecasts:\n",
    "            final_forecasts.append(f.median)\n",
    "\n",
    "        mase_metrics = []\n",
    "        for item_id, ts in enumerate(test):\n",
    "          training_data = ts[\"target\"].T[:-self.prediction_length]\n",
    "          ground_truth = ts[\"target\"].T[-self.prediction_length:]\n",
    "\n",
    "          y_pred_naive = np.array(training_data)[:-int(self.seasonality)]\n",
    "          mae_naive = mean_absolute_error(np.array(training_data)[int(self.seasonality):], y_pred_naive, multioutput=\"uniform_average\")\n",
    "\n",
    "          mae_score = mean_absolute_error(\n",
    "              np.array(ground_truth),\n",
    "              final_forecasts[item_id],\n",
    "              sample_weight=None,\n",
    "              multioutput=\"uniform_average\",\n",
    "          )\n",
    "\n",
    "          epsilon = np.finfo(np.float64).eps\n",
    "          if mae_naive == 0:\n",
    "            continue\n",
    "          mase_score = mae_score / np.maximum(mae_naive, epsilon)\n",
    "\n",
    "\n",
    "          mase_metrics.append(mase_score)\n",
    "\n",
    "        \n",
    "        print(\"MINE\")\n",
    "        print(np.mean(mase_metrics))\n",
    "\n",
    "        # print(\"GLUONTS\")\n",
    "        \n",
    "        # tss = list(ts_it)\n",
    "        # if self.multivariate:\n",
    "        #     evaluator = MultivariateEvaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "        # else:\n",
    "        #     evaluator = Evaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "\n",
    "        # agg_metrics, item_metrics = evaluator(tss, forecasts)  # type: ignore # we are sure that tss is a list of DataFrame in multivariate case\n",
    "        # # print(json.dumps(agg_metrics, indent=4))\n",
    "        # # print(agg_metrics)\n",
    "        # print(item_metrics.mean())\n",
    "\n",
    "        if not save:\n",
    "            return np.mean(mase_metrics)\n",
    "      \n",
    "        # make directory called saved_nonorm_{self.model}_{self.dataset_name}\n",
    "        dir_name = f'saved_POINT_GAS_{self.model}_{self.dataset_name}_mean_str_{self.mean_str}_var_str_{self.var_str}'\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "        return np.mean(mase_metrics), predictor, dir_name, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-09 13:25:15,001] A new study created in memory with name: no-name-eb3d6be8-96a8-4ded-bebe-6074dd71d523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 144 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 144 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Normalizing train dataset...\n",
      "Saving normalizer parameters...\n",
      "Saving normalized train dataset, means and vars...\n",
      "Done.\n",
      "Normalizing test dataset...\n",
      "Done.\n",
      "Saving normalized test dataset, means and vars...\n",
      "Initializing the mean linear layer...\n",
      "Initializing the estimator...\n",
      "Training the estimator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 25.14it/s, epoch=1/5, avg_epoch_loss=0.0837]\n",
      "100%|██████████| 100/100 [00:03<00:00, 25.72it/s, epoch=2/5, avg_epoch_loss=0.0601]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.09it/s, epoch=3/5, avg_epoch_loss=0.047]\n",
      "100%|██████████| 100/100 [00:03<00:00, 27.51it/s, epoch=4/5, avg_epoch_loss=0.0423]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.66it/s, epoch=5/5, avg_epoch_loss=0.0375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Evaluating the estimator...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 3it [00:11,  3.74s/it]\n",
      "/tmp/ipykernel_143747/4189682787.py:108: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"trainer:learning_rate\": trial.suggest_loguniform(\"trainer:learning_rate\", 1e-5, 1e-1),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  item_id         forecast_start       MSE  abs_error  abs_target_sum  \\\n",
      "0    None  1998-03-23/1998-03-29  0.012700   0.636357       12.471882   \n",
      "1    None  1998-03-23/1998-03-29  0.030339   0.854442        9.470610   \n",
      "2    None  1998-03-23/1998-03-29  0.021980   0.988514       11.196161   \n",
      "\n",
      "   abs_target_mean  seasonal_error      MASE      MAPE     sMAPE  \\\n",
      "0         1.558985        0.108730  0.731579  0.049151  0.051098   \n",
      "1         1.183826        0.122170  0.874232  0.086990  0.098566   \n",
      "2         1.399520        0.119135  1.037178  0.088496  0.091115   \n",
      "\n",
      "   num_masked_target_values        ND       MSIS  QuantileLoss[0.1]  \\\n",
      "0                       0.0  0.051023  29.263178           0.339270   \n",
      "1                       0.0  0.090220  34.969294           0.171291   \n",
      "2                       0.0  0.088290  41.487134           0.640791   \n",
      "\n",
      "   Coverage[0.1]  QuantileLoss[0.5]  Coverage[0.5]  QuantileLoss[0.9]  \\\n",
      "0          0.250           0.636357          0.250           0.933444   \n",
      "1          0.125           0.854442          0.125           1.537593   \n",
      "2          0.250           0.988514          0.250           1.336238   \n",
      "\n",
      "   Coverage[0.9]  \n",
      "0          0.250  \n",
      "1          0.125  \n",
      "2          0.250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 27.52it/s, epoch=1/13, avg_epoch_loss=0.1]\n",
      "1it [00:00, 70.86it/s, epoch=1/13, validation_avg_epoch_loss=0.0667]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.17it/s, epoch=2/13, avg_epoch_loss=0.0913]\n",
      "1it [00:00, 70.69it/s, epoch=2/13, validation_avg_epoch_loss=0.0738]\n",
      "100%|██████████| 100/100 [00:03<00:00, 27.69it/s, epoch=3/13, avg_epoch_loss=0.0868]\n",
      "1it [00:00, 80.70it/s, epoch=3/13, validation_avg_epoch_loss=0.0839]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.49it/s, epoch=4/13, avg_epoch_loss=0.0873]\n",
      "1it [00:00, 82.38it/s, epoch=4/13, validation_avg_epoch_loss=0.0705]\n",
      "100%|██████████| 100/100 [00:03<00:00, 27.82it/s, epoch=5/13, avg_epoch_loss=0.0892]\n",
      "1it [00:00, 90.89it/s, epoch=5/13, validation_avg_epoch_loss=0.0775]\n",
      "100%|██████████| 100/100 [00:03<00:00, 27.51it/s, epoch=6/13, avg_epoch_loss=0.0908]\n",
      "1it [00:00, 78.87it/s, epoch=6/13, validation_avg_epoch_loss=0.0791]\n",
      "100%|██████████| 100/100 [00:03<00:00, 27.02it/s, epoch=7/13, avg_epoch_loss=0.0898]\n",
      "1it [00:00, 12.97it/s, epoch=7/13, validation_avg_epoch_loss=0.0909]\n",
      "100%|██████████| 100/100 [00:03<00:00, 29.01it/s, epoch=8/13, avg_epoch_loss=0.0878]\n",
      "1it [00:00, 12.72it/s, epoch=8/13, validation_avg_epoch_loss=0.0701]\n",
      "100%|██████████| 100/100 [00:03<00:00, 29.55it/s, epoch=9/13, avg_epoch_loss=0.0874]\n",
      "1it [00:00, 79.07it/s, epoch=9/13, validation_avg_epoch_loss=0.0706]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.58it/s, epoch=10/13, avg_epoch_loss=0.0886]\n",
      "1it [00:00, 82.40it/s, epoch=10/13, validation_avg_epoch_loss=0.073]\n",
      "100%|██████████| 100/100 [00:03<00:00, 27.61it/s, epoch=11/13, avg_epoch_loss=0.0888]\n",
      "1it [00:00, 25.69it/s, epoch=11/13, validation_avg_epoch_loss=0.0772]\n",
      "100%|██████████| 100/100 [00:03<00:00, 29.63it/s, epoch=12/13, avg_epoch_loss=0.0874]\n",
      "1it [00:00, 88.98it/s, epoch=12/13, validation_avg_epoch_loss=0.0768]\n",
      "100%|██████████| 100/100 [00:03<00:00, 28.59it/s, epoch=13/13, avg_epoch_loss=0.0844]\n",
      "1it [00:00, 13.66it/s, epoch=13/13, validation_avg_epoch_loss=0.0816]\n",
      "[I 2024-02-09 13:26:40,528] Trial 0 finished with value: 0.3241345469404516 and parameters: {'num_layers': 3, 'hidden_dim_0': 51, 'hidden_dim_1': 22, 'hidden_dim_2': 22, 'trainer:learning_rate': 0.03364901593602788, 'trainer:epochs': 13}. Best is trial 0 with value: 0.3241345469404516.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINE\n",
      "0.3241345469404516\n"
     ]
    }
   ],
   "source": [
    "DATASET_NAME, model_choice, ctx, DATASET_FILE_FOLDER, n_trials, mean_str, var_str = 'nn5_weekly', 'feedforward', 'gpu', None, 1, 0.1, 0.1\n",
    "multivariate = False\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "obj = Objective(\n",
    "        model_choice,DATASET_NAME, ctx, DATASET_FILE_FOLDER, multivariate, mean_str, var_str\n",
    "    )\n",
    "study.optimize(\n",
    "    obj,\n",
    "    n_trials=n_trials,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
