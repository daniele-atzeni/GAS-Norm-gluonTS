{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clts2/lib/python3.10/site-packages/gluonts/json.py:101: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/clts2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils import get_dataset_from_file\n",
    "from run_experiment.gas_experiment import run_gas_experiment\n",
    "from default_parameters import *\n",
    "\n",
    "from gluonts.mx.distribution import StudentTOutput, MultivariateGaussianOutput\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator, MultivariateEvaluator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.mx.trainer.callback import TrainingHistory\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "import mxnet as mx\n",
    "# from my_models.gluonts_models.univariate.feedforward_linear_means._estimator import (\n",
    "#     SimpleFeedForwardEstimator as FF_gluonts_univariate_linear,\n",
    "# )\n",
    "from my_models.gluonts_models.univariate.probabilistic_forecast.feedforward_gas_means._estimator import (\n",
    "    SimpleFeedForwardEstimator as FF_gluonts_univariate_gas,\n",
    ")\n",
    "from my_models.gluonts_models.feedforward_multivariate_linear_means._estimator import (\n",
    "    SimpleFeedForwardEstimator as FF_gluonts_multivariate_linear,\n",
    ")\n",
    "from my_models.gluonts_models.univariate.point_forecast.feedforward_gas_means._estimator import (\n",
    "    SimpleFeedForwardEstimator as FF_gluonts_univariate_gas_point,\n",
    ")\n",
    "# from my_models.gluonts_models.multivariate_feedforward_gas_means._estimator import (\n",
    "#     SimpleFeedForwardEstimator as FF_gluonts_multivariate_gas,\n",
    "# )\n",
    "\n",
    "from my_models.gluonts_models.univariate.probabilistic_forecast.deepar_gas_means._estimator import (\n",
    "    DeepAREstimator as DeepAR_gluonts_univariate_gas,\n",
    ")\n",
    "from my_models.gluonts_models.univariate.wavenet_gas_means._estimator import (\n",
    "    WaveNetEstimator as WaveNet_gluonts_gas_means,\n",
    ")\n",
    "from my_models.gluonts_models.univariate.probabilistic_forecast.transformer_gas_means._estimator import (\n",
    "    TransformerEstimator as Transformer_gluonts_gas_means,\n",
    ")\n",
    "from my_models.gluonts_models.univariate.point_forecast.transformer_gas_means._estimator import (\n",
    "    TransformerEstimator as Transformer_gluonts_gas_means_point,\n",
    ")\n",
    "from my_models.gluonts_models.univariate.probabilistic_forecast.transformer_test._estimator import (\n",
    "    TransformerEstimator as Transformer_test,\n",
    ")\n",
    "from my_models.gluonts_models.univariate.probabilistic_forecast.seq2seq._mq_dnn_estimator import (\n",
    "    MQCNNEstimator as MQCNN_gluonts_univariate_gas,\n",
    ")\n",
    "\n",
    "from run_experiment.dl_model_experiment import GasHybridBlock\n",
    "from normalizer import GASNormalizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import time\n",
    "import optuna\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking hybrid forward inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 112 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of 107 | elapsed:    2.4s remaining:   49.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of 107 | elapsed:    2.6s remaining:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of 107 | elapsed:    2.6s remaining:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of 107 | elapsed:    2.7s remaining:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of 107 | elapsed:    2.7s remaining:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of 107 | elapsed:    2.8s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  71 out of 107 | elapsed:    2.8s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  82 out of 107 | elapsed:    2.8s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of 107 | elapsed:    2.8s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 104 out of 107 | elapsed:    2.8s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 107 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 112 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of 107 | elapsed:    0.7s remaining:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of 107 | elapsed:    0.7s remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of 107 | elapsed:    0.8s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of 107 | elapsed:    0.8s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of 107 | elapsed:    0.8s remaining:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of 107 | elapsed:    0.8s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  71 out of 107 | elapsed:    0.8s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  82 out of 107 | elapsed:    0.8s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of 107 | elapsed:    0.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 104 out of 107 | elapsed:    0.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 107 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Normalizing train dataset...\n",
      "Saving normalizer parameters...\n",
      "Saving normalized train dataset, means and vars...\n",
      "Done.\n",
      "Normalizing test dataset...\n",
      "Done.\n",
      "Saving normalized test dataset, means and vars...\n",
      "Initializing the mean linear layer...\n",
      "Initializing the estimator...\n",
      "Training the estimator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:06:36] /work/mxnet/src/base.cc:79: cuDNN lib mismatch: linked-against version 8101 != compiled-against version 8500.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.\n",
      "100%|██████████| 100/100 [00:04<00:00, 23.55it/s, epoch=1/1, avg_epoch_loss=3.61]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Evaluating the estimator...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL, multivariate, ctx, DATASET_NAME, DATASET_FILE_FOLDER, mean_str, var_str = 'transformer', False, 'gpu', 'fred_md', None, 0, 0\n",
    "model = MODEL\n",
    "multivariate = multivariate\n",
    "ctx = ctx\n",
    "dataset_name = DATASET_NAME\n",
    "\n",
    "\n",
    "\n",
    "DATASET_TYPE = \"gluonts\"  # \"synthetic\"\n",
    "DATASET_PARAMS = real_world_data_params  # synthetic_generation_params\n",
    "DATASET_PARAMS[\"multivariate\"] = multivariate\n",
    "# DATASET_FILE_FOLDER = None #'tsf_data'\n",
    "\n",
    "NORMALIZER_NAME = \"gas_t_student\"  # \"gas_simple_gaussian\", \"gas_complex_gaussian\"\n",
    "NORMALIZER_INITIAL_GUESSES = gas_t_stud_initial_guesses  # gas_{name}_*\n",
    "NORMALIZER_BOUNDS = gas_t_stud_bounds\n",
    "NORMALIZER_PARAMS = gas_t_stud_params\n",
    "NORMALIZER_PARAMS['mean_strength'] = mean_str\n",
    "NORMALIZER_PARAMS['var_strength'] = var_str\n",
    "\n",
    "\n",
    "MEAN_LAYER_NAME = \"gas\"  # TODO: gas\n",
    "MEAN_LAYER_PARAMS = gas_mean_layer_params\n",
    "\n",
    "DL_MODEL_LIBRARY = \"gluonts\"  # \"torch\"\n",
    "DL_MODEL_NAME = MODEL  # TODO: \"transformer\"\n",
    "if model == 'feedforward':\n",
    "    DL_MODEL_PARAMS = gluonts_feedforward_params\n",
    "elif model == 'transformer':\n",
    "    DL_MODEL_PARAMS = gluonts_transformer_params\n",
    "elif model == 'deepar':\n",
    "    DL_MODEL_PARAMS = gluonts_deepar_params\n",
    "elif model == 'wavenet':\n",
    "    DL_MODEL_PARAMS = gluonts_wavenet_params\n",
    "elif model == 'mqcnn':\n",
    "    DL_MODEL_PARAMS = gluonts_mqcnn_params\n",
    "\n",
    "N_TRAINING_SAMPLES = 5000\n",
    "N_TEST_SAMPLES = 1000\n",
    "\n",
    "ROOT_FOLDER = (\n",
    "    f\"RESULTS_{DATASET_NAME}_{model}_{NORMALIZER_NAME}_{MEAN_LAYER_NAME}_{DL_MODEL_LIBRARY}_{float(mean_str)}_{float(var_str)}\"\n",
    ")\n",
    "if DATASET_PARAMS[\"multivariate\"]:\n",
    "    ROOT_FOLDER += \"_multivariate\"\n",
    "\n",
    "# run for the first time to get all the means and stuff\n",
    "training_params = run_gas_experiment(\n",
    "    DATASET_NAME,\n",
    "    DATASET_TYPE,\n",
    "    DATASET_PARAMS,\n",
    "    ROOT_FOLDER,\n",
    "    NORMALIZER_NAME,\n",
    "    NORMALIZER_INITIAL_GUESSES,\n",
    "    NORMALIZER_BOUNDS,\n",
    "    MEAN_LAYER_NAME,\n",
    "    DL_MODEL_LIBRARY,\n",
    "    DL_MODEL_NAME,\n",
    "    DATASET_FILE_FOLDER,\n",
    "    NORMALIZER_PARAMS,\n",
    "    MEAN_LAYER_PARAMS,\n",
    "    DL_MODEL_PARAMS,\n",
    "    N_TRAINING_SAMPLES,\n",
    "    N_TEST_SAMPLES,\n",
    "    probabilistic=True,\n",
    ")\n",
    "\n",
    "# get the parameters needed to run the model part\n",
    "\n",
    "n_features, context_length, prediction_length, freq, dataset, mean_layer, dl_model_name, dl_model_params, folders = training_params\n",
    "n_train, n_test = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manager import GluonTSDataManager\n",
    "\n",
    "data_manager = GluonTSDataManager(DATASET_NAME, multivariate, DATASET_FILE_FOLDER, False)\n",
    "data_manager = data_manager\n",
    "n_features = data_manager.n_features\n",
    "context_length = data_manager.context_length\n",
    "prediction_length = data_manager.prediction_length\n",
    "nn_train = data_manager.train_dataset\n",
    "nn_test = data_manager.test_dataset\n",
    "freq = data_manager.freq\n",
    "\n",
    "n_train_sub = []\n",
    "for ts in n_train:\n",
    "    n_train_sub.append(\n",
    "        {\n",
    "            \"start\": ts[\"start\"],\n",
    "            \"target\": ts[\"target\"],\n",
    "            # \"feat_dynamic_real\": np.zeros_like(ts[\"feat_dynamic_real\"]),\n",
    "            # \"feat_static_real\": np.zeros_like(ts[\"feat_static_real\"]),\n",
    "        }\n",
    "    )\n",
    "n_test_sub = []\n",
    "for ts in n_test:\n",
    "    n_test_sub.append(\n",
    "        {\n",
    "            \"start\": ts[\"start\"],\n",
    "            \"target\": ts[\"target\"],\n",
    "            # \"feat_dynamic_real\": np.zeros_like(ts[\"feat_dynamic_real\"]),\n",
    "            # \"feat_static_real\": np.zeros_like(ts[\"feat_static_real\"]),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet\n",
    "\n",
    "def are_dicts_equal(dict1, dict2):\n",
    "    if dict1.keys() != dict2.keys():\n",
    "        print(dict1.keys(), dict2.keys())\n",
    "        return False\n",
    "\n",
    "    for key in dict1.keys():\n",
    "        if (isinstance(dict1[key], np.ndarray) and isinstance(dict2[key], np.ndarray)) or (isinstance(dict1[key], mxnet.ndarray.ndarray.NDArray) and isinstance(dict2[key], mxnet.ndarray.ndarray.NDArray)):\n",
    "            if not np.array_equal(dict1[key], dict2[key]):\n",
    "                print(key)\n",
    "                return False\n",
    "        elif dict1[key] != dict2[key]:\n",
    "            print(key)\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def are_lists_equal(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        print(len(list1), len(list2))\n",
    "        return False\n",
    "    i = 0\n",
    "    for dict1, dict2 in zip(list1, list2):\n",
    "        if not are_dicts_equal(dict1, dict2):\n",
    "            print(i)\n",
    "            return False\n",
    "        i += 1\n",
    "\n",
    "    return True\n",
    "\n",
    "# are_lists_equal(n_train_sub, nn_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle file called \"params.pickle\"\n",
    "with open('params.pickle', \"rb\") as f:\n",
    "    n_params = pickle.load(f)\n",
    "\n",
    "with open('params_default.pickle', \"rb\") as f:\n",
    "    nn_params = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are_dicts_equal(n_params[-1], nn_params[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "102\n",
      "dict_keys(['feat_static_cat', 'past_time_feat', 'past_target', 'past_observed_values', 'future_time_feat', 'future_target', 'future_observed_values', 'past_feat_dynamic_real', 'feat_static_real'])\n",
      "dict_keys(['feat_static_cat', 'past_time_feat', 'past_target', 'past_observed_values', 'future_time_feat', 'future_target', 'future_observed_values'])\n",
      "dict_keys(['feat_static_cat', 'past_time_feat', 'past_target', 'past_observed_values', 'future_time_feat', 'future_target', 'future_observed_values'])\n",
      "dict_keys(['feat_static_cat', 'past_time_feat', 'past_target', 'past_observed_values', 'future_time_feat', 'future_target', 'future_observed_values'])\n"
     ]
    }
   ],
   "source": [
    "# print the lens\n",
    "print(len(n_params))\n",
    "print(len(nn_params))\n",
    "\n",
    "# print the keys of both dictionaries\n",
    "print(n_params[0].keys())\n",
    "print(nn_params[0].keys())\n",
    "\n",
    "# limit n_params to only the ones that are in nn_params\n",
    "# n_params = {k: v for k, v in n_params.items() if k in nn_params} # for direct dict\n",
    "n_params = [{k: v for k, v in params.items() if k in nn_params[0]} for params in n_params] # for list of dicts\n",
    "\n",
    "# print the keys of both dictionaries\n",
    "print(n_params[0].keys())\n",
    "print(nn_params[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/clts2/lib/python3.10/site-packages/numpy/core/numeric.py:2457: VisibleDeprecationWarning: Creating an ndarray from nested sequences exceeding the maximum number of dimensions of 32 is deprecated. If you mean to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  a1, a2 = asarray(a1), asarray(a2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_time_feat\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are_lists_equal(n_params, nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_time_feat\n",
      "0\n",
      "past_time_feat\n",
      "1\n",
      "past_time_feat\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(n_params)):\n",
    "    if not are_dicts_equal(n_params[i], nn_params[i]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def combine(params):\n",
    "    # Assuming n_params is your list of dictionaries\n",
    "    combined = defaultdict(list)\n",
    "\n",
    "    for d in params:\n",
    "        for key, value in d.items():\n",
    "            combined[key].append(value.asnumpy())\n",
    "\n",
    "    # Convert lists of arrays to single arrays\n",
    "    for key, value in combined.items():\n",
    "        combined[key] = np.concatenate(value, axis=0)\n",
    "        combined[key]=combined[key].squeeze().reshape(1,-1)\n",
    "        # sort the values in the key\n",
    "        combined[key] = np.sort(combined[key])\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params_combined = combine(n_params)\n",
    "# save the combined params\n",
    "with open('n_params_combined.pickle', \"wb\") as f:\n",
    "    pickle.dump(n_params_combined, f)\n",
    "nn_params_combined = combine(nn_params)\n",
    "# save the combined params\n",
    "with open('nn_params_default_combined.pickle', \"wb\") as f:\n",
    "    pickle.dump(nn_params_combined, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "dict_keys(['feat_static_cat', 'past_time_feat', 'past_target', 'past_observed_values', 'future_time_feat', 'future_target', 'future_observed_values'])\n",
      "dict_keys(['feat_static_cat', 'past_time_feat', 'past_target', 'past_observed_values', 'future_time_feat', 'future_target', 'future_observed_values'])\n",
      "dict_keys(['feat_static_cat', 'past_time_feat', 'past_target', 'past_observed_values', 'future_time_feat', 'future_target', 'future_observed_values'])\n",
      "dict_keys(['feat_static_cat', 'past_time_feat', 'past_target', 'past_observed_values', 'future_time_feat', 'future_target', 'future_observed_values'])\n"
     ]
    }
   ],
   "source": [
    "# print the lens\n",
    "print(len(n_params_combined))\n",
    "print(len(nn_params_combined))\n",
    "\n",
    "# print the keys of both dictionaries\n",
    "print(n_params_combined.keys())\n",
    "print(nn_params_combined.keys())\n",
    "\n",
    "# limit n_params to only the ones that are in nn_params\n",
    "# n_params_combined = {k: v for k, v in n_params_combined.items() if k in nn_params_combined} # for direct dict\n",
    "# n_params_combined = [{k: v for k, v in params.items() if k in nn_params_combined[0]} for params in n_params_combined] # for list of dicts\n",
    "\n",
    "# print the keys of both dictionaries\n",
    "print(n_params_combined.keys())\n",
    "print(nn_params_combined.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past_time_feat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "are_dicts_equal(n_params_combined, nn_params_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrays are equal.\n",
      "past_time_feat Arrays are not equal. 173519 out of 336232 elements are different.\n",
      "Differences at indices: (array([ 13620,  13621,  13622, ..., 336214, 336215, 336216]),)\n",
      "array1 values: [-0.5       -0.5       -0.5       ...  2.8407333  2.8407333  2.8407333]\n",
      "array2 values: [-0.4090909 -0.4090909 -0.4090909 ...  2.8401062  2.8401062  2.8401062]\n",
      "past_target Arrays are not equal. 161617 out of 168116 elements are different.\n",
      "Differences at indices: (array([  5097,   5098,   5099, ..., 168098, 168099, 168100]),)\n",
      "array1 values: [0.9757815 0.9757815 0.9757815 ... 6.202237  6.202237  6.202237 ]\n",
      "array2 values: [0.       0.       0.       ... 6.196274 6.196274 6.196274]\n",
      "past_observed_values Arrays are not equal. 1266 out of 168116 elements are different.\n",
      "Differences at indices: (array([5097, 5098, 5099, ..., 6360, 6361, 6362]),)\n",
      "array1 values: [1. 1. 1. ... 1. 1. 1.]\n",
      "array2 values: [0. 0. 0. ... 0. 0. 0.]\n",
      "future_time_feat Arrays are not equal. 37364 out of 77592 elements are different.\n",
      "Differences at indices: (array([29099, 29100, 32332, ..., 77575, 77576, 77577]),)\n",
      "array1 values: [0.30103   0.30103   0.3181818 ... 2.848189  2.848189  2.848189 ]\n",
      "array2 values: [0.3181818 0.3181818 0.4090909 ... 2.8475728 2.8475728 2.8475728]\n",
      "future_target Arrays are not equal. 37866 out of 38796 elements are different.\n",
      "Differences at indices: (array([    2,     3,     8, ..., 38779, 38780, 38781]),)\n",
      "array1 values: [0.9757815 0.9757815 0.9796274 ... 6.390222  6.390222  6.390222 ]\n",
      "array2 values: [0.9796274 0.9796274 0.9859477 ... 6.3763556 6.3763556 6.3763556]\n",
      "Arrays are equal.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming array1 and array2 are your numpy arrays\n",
    "for key in n_params_combined.keys():\n",
    "    array1 = n_params_combined[key].squeeze()\n",
    "    array2 = nn_params_combined[key].squeeze()\n",
    "    diff_indices = np.where(array1 != array2)\n",
    "\n",
    "    if diff_indices[0].size:\n",
    "        print(f\"{key} Arrays are not equal. {len(diff_indices[0])} out of {len(array1)} elements are different.\")\n",
    "        print(\"Differences at indices:\", diff_indices)\n",
    "        print(\"array1 values:\", array1[diff_indices])\n",
    "        print(\"array2 values:\", array2[diff_indices])\n",
    "    else:\n",
    "        print(\"Arrays are equal.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking mean vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%cd /mnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_0.pkl', 'rb') as f:\n",
    "    train_0 = pickle.load(f).dataset\n",
    "with open('train_1.pkl', 'rb') as f:\n",
    "    train_1 = pickle.load(f).dataset\n",
    "with open('train_2.pkl', 'rb') as f:\n",
    "    train_2 = pickle.load(f).dataset\n",
    "with open('train_3.pkl', 'rb') as f:\n",
    "    train_3 = pickle.load(f).dataset\n",
    "with open('train_4.pkl', 'rb') as f:\n",
    "    train_4 = pickle.load(f).dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9999911e-01 9.9999809e-01 9.9999708e-01 ... 9.9927664e-01\n",
      "  9.9927562e-01 9.9927461e-01]\n",
      " [1.8566617e-04 1.8668659e-04 1.8770700e-04 ... 9.1297494e-04\n",
      "  9.1399468e-04 9.1501430e-04]] \n",
      "\n",
      "[[9.9999911e-01 9.9999809e-01 9.9999708e-01 ... 9.9927664e-01\n",
      "  9.9927562e-01 9.9927461e-01]\n",
      " [1.8566617e-04 1.8668659e-04 1.8770700e-04 ... 9.1297494e-04\n",
      "  9.1399468e-04 9.1501430e-04]] \n",
      "\n",
      "[[9.9999911e-01 9.9999809e-01 9.9999708e-01 ... 9.9927664e-01\n",
      "  9.9927562e-01 9.9927461e-01]\n",
      " [1.8566617e-04 1.8668659e-04 1.8770700e-04 ... 9.1297494e-04\n",
      "  9.1399468e-04 9.1501430e-04]] \n",
      "\n",
      "[[9.9999911e-01 9.9999809e-01 9.9999708e-01 ... 9.9927664e-01\n",
      "  9.9927562e-01 9.9927461e-01]\n",
      " [1.8566617e-04 1.8668659e-04 1.8770700e-04 ... 9.1297494e-04\n",
      "  9.1399468e-04 9.1501430e-04]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_0[0]['means_vars'],'\\n')\n",
    "print(train_1[0]['means_vars'],'\\n')\n",
    "print(train_2[0]['means_vars'],'\\n')\n",
    "print(train_3[0]['means_vars'],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through train_0 and train_1 and compare the means_vars\n",
    "array1 = train_0\n",
    "array2 = train_4 \n",
    "for i in range(len(array1)):\n",
    "    if not np.array_equal(array1[i]['means_vars'], array2[i]['means_vars']):\n",
    "        print('means_vars',i)\n",
    "        print(array1[i]['means_vars'])\n",
    "        print(array2[i]['means_vars'])\n",
    "    if not np.array_equal(array1[i]['target'], array2[i]['target']):\n",
    "        print('target',i)\n",
    "        print(array1[i]['target'])\n",
    "        print(array2[i]['target'])\n",
    "    if not np.array_equal(array1[i]['start'], array2[i]['start']):\n",
    "        print('start',i)\n",
    "        print(array1[i]['start'])\n",
    "        print(array2[i]['start'])\n",
    "    if not np.array_equal(array1[i]['gas_params'], array2[i]['gas_params']):\n",
    "        print('gas_params', i)\n",
    "        print(array1[i]['gas_params'])\n",
    "        print(array2[i]['gas_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manager import GluonTSDataManager\n",
    "\n",
    "data_manager = GluonTSDataManager('fred_md', False, None, False)\n",
    "data_manager = data_manager\n",
    "n_features = data_manager.n_features\n",
    "context_length = data_manager.context_length\n",
    "prediction_length = data_manager.prediction_length\n",
    "nn_train = data_manager.train_dataset\n",
    "nn_test = data_manager.test_dataset\n",
    "freq = data_manager.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6153463"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.train_dataset[0]['target'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clts2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
